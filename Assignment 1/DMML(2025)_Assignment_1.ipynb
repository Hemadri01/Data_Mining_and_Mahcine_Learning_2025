{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DMML(2025) Assignment 1 submission for\n",
        "\n",
        "1. Lucky Mathias Kispotta (MCS202411)\n",
        "2. Hemadri Shekhar Das (MCS 202405)"
      ],
      "metadata": {
        "id": "5IQU8MWeasWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting Dataset from Google Drive\n",
        "\n",
        "The entire dataset was first downloaded and uploaded to a google drive from where it was accessed to run the code."
      ],
      "metadata": {
        "id": "9sqztWOObi0g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2peYdrIQUzxV",
        "outputId": "ae405c91-d0b6-4229-b175-8dedca453b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The entire code is in a single cell. It has been copied to different cells to run the code on different inputs and store the outputs."
      ],
      "metadata": {
        "id": "ZOLid-Job_-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code run for k = 2, f = 500 with output on kos dataset"
      ],
      "metadata": {
        "id": "A4B740FUcNS_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8MquB0-MGPb",
        "outputId": "607cf24e-925c-47f2-9bbf-805dfa60db07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter dataset name (e.g., enron): kos\n",
            "Enter k (size of itemsets): 2\n",
            "Enter min_support (minimum frequency threshold): 500\n",
            "Reading vocabulary file...\n",
            "Vocabulary file loaded. Time taken: 0.01 seconds.\n",
            "Reading docword file (metadata)...\n",
            "Docword file metadata loaded. Time taken: 0.01 seconds.\n",
            "Reading docword file (data)...\n",
            "\n",
            "\n",
            "Docword file data loaded. Time taken: 0.23 seconds.\n",
            "Running Apriori algorithm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 1-itemsets: 100%|\u001b[32m██████████\u001b[0m| 3430/3430 [00:00<00:00, 10513.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating 2-itemsets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 2-itemsets candidates: 100%|\u001b[33m██████████\u001b[0m| 50/50 [00:24<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Frequent Itemsets of size 2 :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Printing itemsets: 100%|\u001b[34m██████████\u001b[0m| 101/101 [00:00<00:00, 90848.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Itemset: {841, 89}, Support: 748\n",
            "Itemset: {2640, 841}, Support: 1250\n",
            "Itemset: {841, 3005}, Support: 873\n",
            "Itemset: {841, 3282}, Support: 654\n",
            "Itemset: {841, 3350}, Support: 556\n",
            "Itemset: {841, 3420}, Support: 1195\n",
            "Itemset: {841, 4143}, Support: 513\n",
            "Itemset: {841, 4196}, Support: 514\n",
            "Itemset: {841, 4735}, Support: 834\n",
            "Itemset: {5185, 841}, Support: 527\n",
            "Itemset: {841, 6659}, Support: 593\n",
            "Itemset: {841, 6689}, Support: 994\n",
            "Itemset: {841, 879}, Support: 648\n",
            "Itemset: {1664, 841}, Support: 937\n",
            "Itemset: {841, 1666}, Support: 783\n",
            "Itemset: {841, 2030}, Support: 766\n",
            "Itemset: {841, 3858}, Support: 721\n",
            "Itemset: {4632, 841}, Support: 876\n",
            "Itemset: {841, 4635}, Support: 660\n",
            "Itemset: {841, 4761}, Support: 610\n",
            "Itemset: {841, 5186}, Support: 794\n",
            "Itemset: {5552, 841}, Support: 652\n",
            "Itemset: {6296, 841}, Support: 726\n",
            "Itemset: {841, 847}, Support: 611\n",
            "Itemset: {841, 4494}, Support: 654\n",
            "Itemset: {841, 5891}, Support: 560\n",
            "Itemset: {5896, 841}, Support: 516\n",
            "Itemset: {841, 4627}, Support: 556\n",
            "Itemset: {841, 4093}, Support: 521\n",
            "Itemset: {2640, 3005}, Support: 643\n",
            "Itemset: {2640, 3420}, Support: 1064\n",
            "Itemset: {2640, 4735}, Support: 563\n",
            "Itemset: {2640, 6659}, Support: 545\n",
            "Itemset: {2640, 6689}, Support: 745\n",
            "Itemset: {2640, 879}, Support: 524\n",
            "Itemset: {2640, 1664}, Support: 763\n",
            "Itemset: {2640, 1666}, Support: 624\n",
            "Itemset: {2640, 2030}, Support: 715\n",
            "Itemset: {2640, 3858}, Support: 588\n",
            "Itemset: {2640, 4632}, Support: 756\n",
            "Itemset: {2640, 4635}, Support: 626\n",
            "Itemset: {2640, 4761}, Support: 500\n",
            "Itemset: {2640, 5186}, Support: 678\n",
            "Itemset: {2640, 5552}, Support: 571\n",
            "Itemset: {2640, 6296}, Support: 531\n",
            "Itemset: {3420, 3005}, Support: 583\n",
            "Itemset: {6689, 3005}, Support: 624\n",
            "Itemset: {1664, 3005}, Support: 693\n",
            "Itemset: {1666, 3005}, Support: 633\n",
            "Itemset: {3005, 2030}, Support: 533\n",
            "Itemset: {3858, 3005}, Support: 516\n",
            "Itemset: {4632, 3005}, Support: 546\n",
            "Itemset: {5186, 3005}, Support: 625\n",
            "Itemset: {5552, 3005}, Support: 616\n",
            "Itemset: {6689, 3282}, Support: 642\n",
            "Itemset: {3420, 3350}, Support: 559\n",
            "Itemset: {3420, 4735}, Support: 506\n",
            "Itemset: {6659, 3420}, Support: 528\n",
            "Itemset: {6689, 3420}, Support: 686\n",
            "Itemset: {3420, 879}, Support: 534\n",
            "Itemset: {1664, 3420}, Support: 922\n",
            "Itemset: {1666, 3420}, Support: 644\n",
            "Itemset: {3420, 2030}, Support: 617\n",
            "Itemset: {3858, 3420}, Support: 610\n",
            "Itemset: {4632, 3420}, Support: 845\n",
            "Itemset: {4635, 3420}, Support: 681\n",
            "Itemset: {4761, 3420}, Support: 695\n",
            "Itemset: {5186, 3420}, Support: 618\n",
            "Itemset: {5552, 3420}, Support: 537\n",
            "Itemset: {6296, 3420}, Support: 516\n",
            "Itemset: {1664, 6659}, Support: 508\n",
            "Itemset: {1664, 6689}, Support: 618\n",
            "Itemset: {6689, 1666}, Support: 537\n",
            "Itemset: {6689, 2030}, Support: 517\n",
            "Itemset: {6689, 3858}, Support: 557\n",
            "Itemset: {4632, 6689}, Support: 543\n",
            "Itemset: {6689, 5186}, Support: 537\n",
            "Itemset: {1664, 879}, Support: 517\n",
            "Itemset: {1664, 1666}, Support: 740\n",
            "Itemset: {1664, 2030}, Support: 606\n",
            "Itemset: {1664, 3858}, Support: 592\n",
            "Itemset: {1664, 4632}, Support: 755\n",
            "Itemset: {1664, 4635}, Support: 598\n",
            "Itemset: {1664, 4761}, Support: 894\n",
            "Itemset: {1664, 5186}, Support: 660\n",
            "Itemset: {1664, 5552}, Support: 608\n",
            "Itemset: {1666, 2030}, Support: 536\n",
            "Itemset: {1666, 3858}, Support: 508\n",
            "Itemset: {4632, 1666}, Support: 562\n",
            "Itemset: {4761, 1666}, Support: 519\n",
            "Itemset: {1666, 5186}, Support: 673\n",
            "Itemset: {5552, 1666}, Support: 553\n",
            "Itemset: {4632, 2030}, Support: 550\n",
            "Itemset: {5186, 2030}, Support: 516\n",
            "Itemset: {4632, 3858}, Support: 518\n",
            "Itemset: {4632, 4635}, Support: 644\n",
            "Itemset: {4632, 4761}, Support: 630\n",
            "Itemset: {4632, 5186}, Support: 538\n",
            "Itemset: {4632, 5552}, Support: 529\n",
            "Itemset: {4761, 4635}, Support: 511\n",
            "Itemset: {5552, 5186}, Support: 555\n",
            "Total Time taken: 28.14 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "import time\n",
        "from pathlib import Path\n",
        "import os\n",
        "import errno\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "\n",
        "class File:\n",
        "    def __init__(self, path, name):\n",
        "        self.name = str(name)\n",
        "        self.vocabFile = \"vocab.\" + self.name + \".txt\"\n",
        "        self.vocabPath = Path(path) / self.vocabFile\n",
        "        self.docWordFile = \"docword.\" + self.name + \".txt.gz\"\n",
        "        self.docWordPath = Path(path) / self.docWordFile\n",
        "\n",
        "        if not self.vocabPath.exists() or self.vocabPath.is_dir():\n",
        "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.vocabPath)\n",
        "        if not self.docWordPath.exists() or self.docWordPath.is_dir():\n",
        "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.docWordPath)\n",
        "\n",
        "    def __read_csv_chunks(self, lines, path, **params):\n",
        "        chunks = []\n",
        "        for i, chunk in enumerate(pd.read_csv(path, **params)):\n",
        "            prog = min((i + 1) / lines * 100.0 * params[\"chunksize\"], 100)\n",
        "            print(f\"Data Load Progress : {prog:.2f} %.\", end=\"\\r\", flush=True)\n",
        "            chunks.append(chunk)\n",
        "        print(\"\\n\")\n",
        "        df = pd.concat(chunks, axis=0)\n",
        "        del chunks\n",
        "        return df\n",
        "\n",
        "    def load_data(self):\n",
        "        start_time = time.time()\n",
        "        print(\"Reading vocabulary file...\")\n",
        "        self.vocab = pd.read_csv(self.vocabPath, header=None, names=[\"word\"])\n",
        "        print(f\"Vocabulary file loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        self.vocab.index += 1\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"Reading docword file (metadata)...\")\n",
        "        tmp = pd.read_csv(self.docWordPath, compression='gzip', header=None, nrows=3)\n",
        "        self.docCount = tmp[0].values[0]\n",
        "        self.wordCount = tmp[0].values[1]\n",
        "        self.NNZ = tmp[0].values[2]\n",
        "        print(f\"Docword file metadata loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"Reading docword file (data)...\")\n",
        "        self.docWord = self.__read_csv_chunks(self.NNZ, self.docWordPath, compression='gzip', header=None, sep=' ',\n",
        "                                              quotechar='\"', on_bad_lines=\"skip\", skiprows=3, chunksize=10000,\n",
        "                                              names=[\"docID\", \"wordID\", \"count\"])\n",
        "        print(f\"Docword file data loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    def get_unique_docs(self):\n",
        "        try:\n",
        "            return self.docIDS\n",
        "        except AttributeError:\n",
        "            docIDS = self.docWord[\"docID\"].unique()\n",
        "            docIDS.sort()\n",
        "            self.docIDS = docIDS\n",
        "            return self.docIDS\n",
        "\n",
        "    def get_words_by_docID(self, _id):\n",
        "        try:\n",
        "            return self.docWordList[_id]\n",
        "        except AttributeError:\n",
        "            return self.docWord[self.docWord['docID'] == _id][\"wordID\"].tolist()\n",
        "\n",
        "\n",
        "# Optimized Apriori algorithm to find frequent itemsets of size k\n",
        "def apriori(dataset, min_support, k):\n",
        "    itemsets = defaultdict(int)\n",
        "\n",
        "    # Generate all itemsets of size 1 (frequent 1-itemsets)\n",
        "    for doc in tqdm(dataset, desc=\"Generating 1-itemsets\", colour=\"green\"):\n",
        "        for word in doc:\n",
        "            itemsets[frozenset([word])] += 1\n",
        "\n",
        "    # Prune infrequent itemsets\n",
        "    itemsets = {itemset: count for itemset, count in itemsets.items() if count >= min_support}\n",
        "\n",
        "    # Start generating k-itemsets\n",
        "    frequent_itemsets = defaultdict(int)\n",
        "    current_itemsets = list(itemsets.keys())\n",
        "\n",
        "    # We only need to keep itemsets of size k, so we stop the generation process once we reach k-itemsets.\n",
        "    for k_itemsets in range(2, k + 1):\n",
        "        print(f\"\\nGenerating {k_itemsets}-itemsets...\")\n",
        "\n",
        "        # Generate candidate itemsets from the previous frequent itemsets\n",
        "        candidate_itemsets = defaultdict(int)\n",
        "\n",
        "        # Join itemsets of size (k_itemsets - 1) to generate k_itemsets\n",
        "        for i in tqdm(range(len(current_itemsets)), desc=f\"Generating {k_itemsets}-itemsets candidates\", colour=\"yellow\"):\n",
        "            for j in range(i + 1, len(current_itemsets)):\n",
        "                # Try to join two itemsets\n",
        "                candidate = current_itemsets[i] | current_itemsets[j]\n",
        "                if len(candidate) == k_itemsets:  # Only consider itemsets of size k_itemsets\n",
        "                    # Candidate itemset must not have any infrequent subsets\n",
        "                    if all(frozenset(comb) in itemsets for comb in combinations(candidate, k_itemsets - 1)):\n",
        "                        for doc in dataset:\n",
        "                            if candidate.issubset(doc):\n",
        "                                candidate_itemsets[candidate] += 1\n",
        "\n",
        "        # Prune infrequent itemsets\n",
        "        candidate_itemsets = {itemset: count for itemset, count in candidate_itemsets.items() if count >= min_support}\n",
        "\n",
        "        if not candidate_itemsets:\n",
        "            break  # No more frequent itemsets can be generated\n",
        "\n",
        "        # Update the frequent itemsets dictionary with only size k itemsets\n",
        "        if k_itemsets == k:\n",
        "            frequent_itemsets.update(candidate_itemsets)\n",
        "\n",
        "        # Update current itemsets for the next iteration\n",
        "        current_itemsets = list(candidate_itemsets.keys())\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "\n",
        "def main(path, name, k, min_support):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize File class to load data\n",
        "    file_handler = File(path, name)\n",
        "\n",
        "    # Load data\n",
        "    file_handler.load_data()\n",
        "\n",
        "    # Get the list of documents as transactions\n",
        "    documents = [file_handler.get_words_by_docID(doc_id) for doc_id in file_handler.get_unique_docs()]\n",
        "\n",
        "    # Apply Apriori algorithm to find frequent itemsets of size k\n",
        "    print(\"Running Apriori algorithm...\")\n",
        "    frequent_itemsets = apriori(documents, min_support, k)\n",
        "\n",
        "    # Print the results for frequent itemsets of size k\n",
        "    print(\"\\nFrequent Itemsets of size\", k, \":\")\n",
        "    if frequent_itemsets:\n",
        "        for itemset, count in tqdm(frequent_itemsets.items(), desc=\"Printing itemsets\", colour=\"blue\"):\n",
        "            print(f\"Itemset: {set(itemset)}, Support: {count}\")\n",
        "    else:\n",
        "        print(f\"No frequent itemsets of size {k} found.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    dataset_name = input(\"Enter dataset name (e.g., enron): \")\n",
        "    k = int(input(\"Enter k (size of itemsets): \"))\n",
        "    min_support = int(input(\"Enter min_support (minimum frequency threshold): \"))\n",
        "    path = r\"/content/drive/MyDrive/bag+of+words\"  # Adjust path if needed\n",
        "    main(path, dataset_name, k, min_support)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code run for k = 2, f = 700 with output on kos dataset"
      ],
      "metadata": {
        "id": "fh9tMpRwcr21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "import time\n",
        "from pathlib import Path\n",
        "import os\n",
        "import errno\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "\n",
        "class File:\n",
        "    def __init__(self, path, name):\n",
        "        self.name = str(name)\n",
        "        self.vocabFile = \"vocab.\" + self.name + \".txt\"\n",
        "        self.vocabPath = Path(path) / self.vocabFile\n",
        "        self.docWordFile = \"docword.\" + self.name + \".txt.gz\"\n",
        "        self.docWordPath = Path(path) / self.docWordFile\n",
        "\n",
        "        if not self.vocabPath.exists() or self.vocabPath.is_dir():\n",
        "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.vocabPath)\n",
        "        if not self.docWordPath.exists() or self.docWordPath.is_dir():\n",
        "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.docWordPath)\n",
        "\n",
        "    def __read_csv_chunks(self, lines, path, **params):\n",
        "        chunks = []\n",
        "        for i, chunk in enumerate(pd.read_csv(path, **params)):\n",
        "            prog = min((i + 1) / lines * 100.0 * params[\"chunksize\"], 100)\n",
        "            print(f\"Data Load Progress : {prog:.2f} %.\", end=\"\\r\", flush=True)\n",
        "            chunks.append(chunk)\n",
        "        print(\"\\n\")\n",
        "        df = pd.concat(chunks, axis=0)\n",
        "        del chunks\n",
        "        return df\n",
        "\n",
        "    def load_data(self):\n",
        "        start_time = time.time()\n",
        "        print(\"Reading vocabulary file...\")\n",
        "        self.vocab = pd.read_csv(self.vocabPath, header=None, names=[\"word\"])\n",
        "        print(f\"Vocabulary file loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        self.vocab.index += 1\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"Reading docword file (metadata)...\")\n",
        "        tmp = pd.read_csv(self.docWordPath, compression='gzip', header=None, nrows=3)\n",
        "        self.docCount = tmp[0].values[0]\n",
        "        self.wordCount = tmp[0].values[1]\n",
        "        self.NNZ = tmp[0].values[2]\n",
        "        print(f\"Docword file metadata loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"Reading docword file (data)...\")\n",
        "        self.docWord = self.__read_csv_chunks(self.NNZ, self.docWordPath, compression='gzip', header=None, sep=' ',\n",
        "                                              quotechar='\"', on_bad_lines=\"skip\", skiprows=3, chunksize=10000,\n",
        "                                              names=[\"docID\", \"wordID\", \"count\"])\n",
        "        print(f\"Docword file data loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    def get_unique_docs(self):\n",
        "        try:\n",
        "            return self.docIDS\n",
        "        except AttributeError:\n",
        "            docIDS = self.docWord[\"docID\"].unique()\n",
        "            docIDS.sort()\n",
        "            self.docIDS = docIDS\n",
        "            return self.docIDS\n",
        "\n",
        "    def get_words_by_docID(self, _id):\n",
        "        try:\n",
        "            return self.docWordList[_id]\n",
        "        except AttributeError:\n",
        "            return self.docWord[self.docWord['docID'] == _id][\"wordID\"].tolist()\n",
        "\n",
        "\n",
        "# Optimized Apriori algorithm to find frequent itemsets of size k\n",
        "def apriori(dataset, min_support, k):\n",
        "    itemsets = defaultdict(int)\n",
        "\n",
        "    # Generate all itemsets of size 1 (frequent 1-itemsets)\n",
        "    for doc in tqdm(dataset, desc=\"Generating 1-itemsets\", colour=\"green\"):\n",
        "        for word in doc:\n",
        "            itemsets[frozenset([word])] += 1\n",
        "\n",
        "    # Prune infrequent itemsets\n",
        "    itemsets = {itemset: count for itemset, count in itemsets.items() if count >= min_support}\n",
        "\n",
        "    # Start generating k-itemsets\n",
        "    frequent_itemsets = defaultdict(int)\n",
        "    current_itemsets = list(itemsets.keys())\n",
        "\n",
        "    # We only need to keep itemsets of size k, so we stop the generation process once we reach k-itemsets.\n",
        "    for k_itemsets in range(2, k + 1):\n",
        "        print(f\"\\nGenerating {k_itemsets}-itemsets...\")\n",
        "\n",
        "        # Generate candidate itemsets from the previous frequent itemsets\n",
        "        candidate_itemsets = defaultdict(int)\n",
        "\n",
        "        # Join itemsets of size (k_itemsets - 1) to generate k_itemsets\n",
        "        for i in tqdm(range(len(current_itemsets)), desc=f\"Generating {k_itemsets}-itemsets candidates\", colour=\"yellow\"):\n",
        "            for j in range(i + 1, len(current_itemsets)):\n",
        "                # Try to join two itemsets\n",
        "                candidate = current_itemsets[i] | current_itemsets[j]\n",
        "                if len(candidate) == k_itemsets:  # Only consider itemsets of size k_itemsets\n",
        "                    # Candidate itemset must not have any infrequent subsets\n",
        "                    if all(frozenset(comb) in itemsets for comb in combinations(candidate, k_itemsets - 1)):\n",
        "                        for doc in dataset:\n",
        "                            if candidate.issubset(doc):\n",
        "                                candidate_itemsets[candidate] += 1\n",
        "\n",
        "        # Prune infrequent itemsets\n",
        "        candidate_itemsets = {itemset: count for itemset, count in candidate_itemsets.items() if count >= min_support}\n",
        "\n",
        "        if not candidate_itemsets:\n",
        "            break  # No more frequent itemsets can be generated\n",
        "\n",
        "        # Update the frequent itemsets dictionary with only size k itemsets\n",
        "        if k_itemsets == k:\n",
        "            frequent_itemsets.update(candidate_itemsets)\n",
        "\n",
        "        # Update current itemsets for the next iteration\n",
        "        current_itemsets = list(candidate_itemsets.keys())\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "\n",
        "def main(path, name, k, min_support):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize File class to load data\n",
        "    file_handler = File(path, name)\n",
        "\n",
        "    # Load data\n",
        "    file_handler.load_data()\n",
        "\n",
        "    # Get the list of documents as transactions\n",
        "    documents = [file_handler.get_words_by_docID(doc_id) for doc_id in file_handler.get_unique_docs()]\n",
        "\n",
        "    # Apply Apriori algorithm to find frequent itemsets of size k\n",
        "    print(\"Running Apriori algorithm...\")\n",
        "    frequent_itemsets = apriori(documents, min_support, k)\n",
        "\n",
        "    # Print the results for frequent itemsets of size k\n",
        "    print(\"\\nFrequent Itemsets of size\", k, \":\")\n",
        "    if frequent_itemsets:\n",
        "        for itemset, count in tqdm(frequent_itemsets.items(), desc=\"Printing itemsets\", colour=\"blue\"):\n",
        "            print(f\"Itemset: {set(itemset)}, Support: {count}\")\n",
        "    else:\n",
        "        print(f\"No frequent itemsets of size {k} found.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    dataset_name = input(\"Enter dataset name (e.g., enron): \")\n",
        "    k = int(input(\"Enter k (size of itemsets): \"))\n",
        "    min_support = int(input(\"Enter min_support (minimum frequency threshold): \"))\n",
        "    path = r\"/content/drive/MyDrive/bag+of+words\"  # Adjust path if needed\n",
        "    main(path, dataset_name, k, min_support)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHZA6y4iIdJH",
        "outputId": "0713d0f5-1d4e-41b5-e9b6-f38164e3937f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter dataset name (e.g., enron): kos\n",
            "Enter k (size of itemsets): 2\n",
            "Enter min_support (minimum frequency threshold): 700\n",
            "Reading vocabulary file...\n",
            "Vocabulary file loaded. Time taken: 0.01 seconds.\n",
            "Reading docword file (metadata)...\n",
            "Docword file metadata loaded. Time taken: 0.01 seconds.\n",
            "Reading docword file (data)...\n",
            "\n",
            "\n",
            "Docword file data loaded. Time taken: 0.21 seconds.\n",
            "Running Apriori algorithm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 1-itemsets: 100%|\u001b[32m██████████\u001b[0m| 3430/3430 [00:00<00:00, 17943.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating 2-itemsets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 2-itemsets candidates: 100%|\u001b[33m██████████\u001b[0m| 31/31 [00:09<00:00,  3.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Frequent Itemsets of size 2 :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Printing itemsets: 100%|\u001b[34m██████████\u001b[0m| 23/23 [00:00<00:00, 45439.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Itemset: {841, 89}, Support: 748\n",
            "Itemset: {2640, 841}, Support: 1250\n",
            "Itemset: {841, 3005}, Support: 873\n",
            "Itemset: {841, 3420}, Support: 1195\n",
            "Itemset: {841, 4735}, Support: 834\n",
            "Itemset: {841, 6689}, Support: 994\n",
            "Itemset: {1664, 841}, Support: 937\n",
            "Itemset: {841, 1666}, Support: 783\n",
            "Itemset: {841, 2030}, Support: 766\n",
            "Itemset: {841, 3858}, Support: 721\n",
            "Itemset: {4632, 841}, Support: 876\n",
            "Itemset: {841, 5186}, Support: 794\n",
            "Itemset: {6296, 841}, Support: 726\n",
            "Itemset: {2640, 3420}, Support: 1064\n",
            "Itemset: {2640, 6689}, Support: 745\n",
            "Itemset: {2640, 1664}, Support: 763\n",
            "Itemset: {2640, 2030}, Support: 715\n",
            "Itemset: {2640, 4632}, Support: 756\n",
            "Itemset: {1664, 3420}, Support: 922\n",
            "Itemset: {4632, 3420}, Support: 845\n",
            "Itemset: {1664, 1666}, Support: 740\n",
            "Itemset: {1664, 4632}, Support: 755\n",
            "Itemset: {1664, 4761}, Support: 894\n",
            "Total Time taken: 12.06 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code run for k = 3, f = 500 with output on kos dataset"
      ],
      "metadata": {
        "id": "jq9o5k45cwgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "import time\n",
        "from pathlib import Path\n",
        "import os\n",
        "import errno\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "\n",
        "class File:\n",
        "    def __init__(self, path, name):\n",
        "        self.name = str(name)\n",
        "        self.vocabFile = \"vocab.\" + self.name + \".txt\"\n",
        "        self.vocabPath = Path(path) / self.vocabFile\n",
        "        self.docWordFile = \"docword.\" + self.name + \".txt.gz\"\n",
        "        self.docWordPath = Path(path) / self.docWordFile\n",
        "\n",
        "        if not self.vocabPath.exists() or self.vocabPath.is_dir():\n",
        "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.vocabPath)\n",
        "        if not self.docWordPath.exists() or self.docWordPath.is_dir():\n",
        "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.docWordPath)\n",
        "\n",
        "    def __read_csv_chunks(self, lines, path, **params):\n",
        "        chunks = []\n",
        "        for i, chunk in enumerate(pd.read_csv(path, **params)):\n",
        "            prog = min((i + 1) / lines * 100.0 * params[\"chunksize\"], 100)\n",
        "            print(f\"Data Load Progress : {prog:.2f} %.\", end=\"\\r\", flush=True)\n",
        "            chunks.append(chunk)\n",
        "        print(\"\\n\")\n",
        "        df = pd.concat(chunks, axis=0)\n",
        "        del chunks\n",
        "        return df\n",
        "\n",
        "    def load_data(self):\n",
        "        start_time = time.time()\n",
        "        print(\"Reading vocabulary file...\")\n",
        "        self.vocab = pd.read_csv(self.vocabPath, header=None, names=[\"word\"])\n",
        "        print(f\"Vocabulary file loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        self.vocab.index += 1\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"Reading docword file (metadata)...\")\n",
        "        tmp = pd.read_csv(self.docWordPath, compression='gzip', header=None, nrows=3)\n",
        "        self.docCount = tmp[0].values[0]\n",
        "        self.wordCount = tmp[0].values[1]\n",
        "        self.NNZ = tmp[0].values[2]\n",
        "        print(f\"Docword file metadata loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"Reading docword file (data)...\")\n",
        "        self.docWord = self.__read_csv_chunks(self.NNZ, self.docWordPath, compression='gzip', header=None, sep=' ',\n",
        "                                              quotechar='\"', on_bad_lines=\"skip\", skiprows=3, chunksize=10000,\n",
        "                                              names=[\"docID\", \"wordID\", \"count\"])\n",
        "        print(f\"Docword file data loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    def get_unique_docs(self):\n",
        "        try:\n",
        "            return self.docIDS\n",
        "        except AttributeError:\n",
        "            docIDS = self.docWord[\"docID\"].unique()\n",
        "            docIDS.sort()\n",
        "            self.docIDS = docIDS\n",
        "            return self.docIDS\n",
        "\n",
        "    def get_words_by_docID(self, _id):\n",
        "        try:\n",
        "            return self.docWordList[_id]\n",
        "        except AttributeError:\n",
        "            return self.docWord[self.docWord['docID'] == _id][\"wordID\"].tolist()\n",
        "\n",
        "\n",
        "# Optimized Apriori algorithm to find frequent itemsets of size k\n",
        "def apriori(dataset, min_support, k):\n",
        "    itemsets = defaultdict(int)\n",
        "\n",
        "    # Generate all itemsets of size 1 (frequent 1-itemsets)\n",
        "    for doc in tqdm(dataset, desc=\"Generating 1-itemsets\", colour=\"green\"):\n",
        "        for word in doc:\n",
        "            itemsets[frozenset([word])] += 1\n",
        "\n",
        "    # Prune infrequent itemsets\n",
        "    itemsets = {itemset: count for itemset, count in itemsets.items() if count >= min_support}\n",
        "\n",
        "    # Start generating k-itemsets\n",
        "    frequent_itemsets = defaultdict(int)\n",
        "    current_itemsets = list(itemsets.keys())\n",
        "\n",
        "    # We only need to keep itemsets of size k, so we stop the generation process once we reach k-itemsets.\n",
        "    for k_itemsets in range(2, k + 1):\n",
        "        print(f\"\\nGenerating {k_itemsets}-itemsets...\")\n",
        "\n",
        "        # Generate candidate itemsets from the previous frequent itemsets\n",
        "        candidate_itemsets = defaultdict(int)\n",
        "\n",
        "        # Join itemsets of size (k_itemsets - 1) to generate k_itemsets\n",
        "        for i in tqdm(range(len(current_itemsets)), desc=f\"Generating {k_itemsets}-itemsets candidates\", colour=\"yellow\"):\n",
        "            for j in range(i + 1, len(current_itemsets)):\n",
        "                # Try to join two itemsets\n",
        "                candidate = current_itemsets[i] | current_itemsets[j]\n",
        "                if len(candidate) == k_itemsets:  # Only consider itemsets of size k_itemsets\n",
        "                    # Candidate itemset must not have any infrequent subsets\n",
        "                    if all(frozenset(comb) in itemsets for comb in combinations(candidate, k_itemsets - 1)):\n",
        "                        for doc in dataset:\n",
        "                            if candidate.issubset(doc):\n",
        "                                candidate_itemsets[candidate] += 1\n",
        "\n",
        "        # Prune infrequent itemsets\n",
        "        candidate_itemsets = {itemset: count for itemset, count in candidate_itemsets.items() if count >= min_support}\n",
        "\n",
        "        if not candidate_itemsets:\n",
        "            break  # No more frequent itemsets can be generated\n",
        "\n",
        "        # Update the frequent itemsets dictionary with only size k itemsets\n",
        "        if k_itemsets == k:\n",
        "            frequent_itemsets.update(candidate_itemsets)\n",
        "\n",
        "        # Update current itemsets for the next iteration\n",
        "        current_itemsets = list(candidate_itemsets.keys())\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "\n",
        "def main(path, name, k, min_support):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize File class to load data\n",
        "    file_handler = File(path, name)\n",
        "\n",
        "    # Load data\n",
        "    file_handler.load_data()\n",
        "\n",
        "    # Get the list of documents as transactions\n",
        "    documents = [file_handler.get_words_by_docID(doc_id) for doc_id in file_handler.get_unique_docs()]\n",
        "\n",
        "    # Apply Apriori algorithm to find frequent itemsets of size k\n",
        "    print(\"Running Apriori algorithm...\")\n",
        "    frequent_itemsets = apriori(documents, min_support, k)\n",
        "\n",
        "    # Print the results for frequent itemsets of size k\n",
        "    print(\"\\nFrequent Itemsets of size\", k, \":\")\n",
        "    if frequent_itemsets:\n",
        "        for itemset, count in tqdm(frequent_itemsets.items(), desc=\"Printing itemsets\", colour=\"blue\"):\n",
        "            print(f\"Itemset: {set(itemset)}, Support: {count}\")\n",
        "    else:\n",
        "        print(f\"No frequent itemsets of size {k} found.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    dataset_name = input(\"Enter dataset name (e.g., enron): \")\n",
        "    k = int(input(\"Enter k (size of itemsets): \"))\n",
        "    min_support = int(input(\"Enter min_support (minimum frequency threshold): \"))\n",
        "    path = r\"/content/drive/MyDrive/bag+of+words\"  # Adjust path if needed\n",
        "    main(path, dataset_name, k, min_support)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoypOuPSIqLY",
        "outputId": "b2569f60-2056-4048-cbc4-2dae9768044d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter dataset name (e.g., enron): kos\n",
            "Enter k (size of itemsets): 3\n",
            "Enter min_support (minimum frequency threshold): 500\n",
            "Reading vocabulary file...\n",
            "Vocabulary file loaded. Time taken: 0.01 seconds.\n",
            "Reading docword file (metadata)...\n",
            "Docword file metadata loaded. Time taken: 0.01 seconds.\n",
            "Reading docword file (data)...\n",
            "\n",
            "\n",
            "Docword file data loaded. Time taken: 0.28 seconds.\n",
            "Running Apriori algorithm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 1-itemsets: 100%|\u001b[32m██████████\u001b[0m| 3430/3430 [00:00<00:00, 18067.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating 2-itemsets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 2-itemsets candidates: 100%|\u001b[33m██████████\u001b[0m| 50/50 [00:24<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating 3-itemsets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 3-itemsets candidates: 100%|\u001b[33m██████████\u001b[0m| 101/101 [00:00<00:00, 19939.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Frequent Itemsets of size 3 :\n",
            "No frequent itemsets of size 3 found.\n",
            "Total Time taken: 27.42 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code run for k = 2, f = 800 with output on nips dataset"
      ],
      "metadata": {
        "id": "khAwFF5ZdA8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "import time\n",
        "from pathlib import Path\n",
        "import os\n",
        "import errno\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "\n",
        "class File:\n",
        "    def __init__(self, path, name):\n",
        "        self.name = str(name)\n",
        "        self.vocabFile = \"vocab.\" + self.name + \".txt\"\n",
        "        self.vocabPath = Path(path) / self.vocabFile\n",
        "        self.docWordFile = \"docword.\" + self.name + \".txt.gz\"\n",
        "        self.docWordPath = Path(path) / self.docWordFile\n",
        "\n",
        "        if not self.vocabPath.exists() or self.vocabPath.is_dir():\n",
        "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.vocabPath)\n",
        "        if not self.docWordPath.exists() or self.docWordPath.is_dir():\n",
        "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.docWordPath)\n",
        "\n",
        "    def __read_csv_chunks(self, lines, path, **params):\n",
        "        chunks = []\n",
        "        for i, chunk in enumerate(pd.read_csv(path, **params)):\n",
        "            prog = min((i + 1) / lines * 100.0 * params[\"chunksize\"], 100)\n",
        "            print(f\"Data Load Progress : {prog:.2f} %.\", end=\"\\r\", flush=True)\n",
        "            chunks.append(chunk)\n",
        "        print(\"\\n\")\n",
        "        df = pd.concat(chunks, axis=0)\n",
        "        del chunks\n",
        "        return df\n",
        "\n",
        "    def load_data(self):\n",
        "        start_time = time.time()\n",
        "        print(\"Reading vocabulary file...\")\n",
        "        self.vocab = pd.read_csv(self.vocabPath, header=None, names=[\"word\"])\n",
        "        print(f\"Vocabulary file loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        self.vocab.index += 1\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"Reading docword file (metadata)...\")\n",
        "        tmp = pd.read_csv(self.docWordPath, compression='gzip', header=None, nrows=3)\n",
        "        self.docCount = tmp[0].values[0]\n",
        "        self.wordCount = tmp[0].values[1]\n",
        "        self.NNZ = tmp[0].values[2]\n",
        "        print(f\"Docword file metadata loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"Reading docword file (data)...\")\n",
        "        self.docWord = self.__read_csv_chunks(self.NNZ, self.docWordPath, compression='gzip', header=None, sep=' ',\n",
        "                                              quotechar='\"', on_bad_lines=\"skip\", skiprows=3, chunksize=10000,\n",
        "                                              names=[\"docID\", \"wordID\", \"count\"])\n",
        "        print(f\"Docword file data loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    def get_unique_docs(self):\n",
        "        try:\n",
        "            return self.docIDS\n",
        "        except AttributeError:\n",
        "            docIDS = self.docWord[\"docID\"].unique()\n",
        "            docIDS.sort()\n",
        "            self.docIDS = docIDS\n",
        "            return self.docIDS\n",
        "\n",
        "    def get_words_by_docID(self, _id):\n",
        "        try:\n",
        "            return self.docWordList[_id]\n",
        "        except AttributeError:\n",
        "            return self.docWord[self.docWord['docID'] == _id][\"wordID\"].tolist()\n",
        "\n",
        "\n",
        "# Optimized Apriori algorithm to find frequent itemsets of size k\n",
        "def apriori(dataset, min_support, k):\n",
        "    itemsets = defaultdict(int)\n",
        "\n",
        "    # Generate all itemsets of size 1 (frequent 1-itemsets)\n",
        "    for doc in tqdm(dataset, desc=\"Generating 1-itemsets\", colour=\"green\"):\n",
        "        for word in doc:\n",
        "            itemsets[frozenset([word])] += 1\n",
        "\n",
        "    # Prune infrequent itemsets\n",
        "    itemsets = {itemset: count for itemset, count in itemsets.items() if count >= min_support}\n",
        "\n",
        "    # Start generating k-itemsets\n",
        "    frequent_itemsets = defaultdict(int)\n",
        "    current_itemsets = list(itemsets.keys())\n",
        "\n",
        "    # We only need to keep itemsets of size k, so we stop the generation process once we reach k-itemsets.\n",
        "    for k_itemsets in range(2, k + 1):\n",
        "        print(f\"\\nGenerating {k_itemsets}-itemsets...\")\n",
        "\n",
        "        # Generate candidate itemsets from the previous frequent itemsets\n",
        "        candidate_itemsets = defaultdict(int)\n",
        "\n",
        "        # Join itemsets of size (k_itemsets - 1) to generate k_itemsets\n",
        "        for i in tqdm(range(len(current_itemsets)), desc=f\"Generating {k_itemsets}-itemsets candidates\", colour=\"yellow\"):\n",
        "            for j in range(i + 1, len(current_itemsets)):\n",
        "                # Try to join two itemsets\n",
        "                candidate = current_itemsets[i] | current_itemsets[j]\n",
        "                if len(candidate) == k_itemsets:  # Only consider itemsets of size k_itemsets\n",
        "                    # Candidate itemset must not have any infrequent subsets\n",
        "                    if all(frozenset(comb) in itemsets for comb in combinations(candidate, k_itemsets - 1)):\n",
        "                        for doc in dataset:\n",
        "                            if candidate.issubset(doc):\n",
        "                                candidate_itemsets[candidate] += 1\n",
        "\n",
        "        # Prune infrequent itemsets\n",
        "        candidate_itemsets = {itemset: count for itemset, count in candidate_itemsets.items() if count >= min_support}\n",
        "\n",
        "        if not candidate_itemsets:\n",
        "            break  # No more frequent itemsets can be generated\n",
        "\n",
        "        # Update the frequent itemsets dictionary with only size k itemsets\n",
        "        if k_itemsets == k:\n",
        "            frequent_itemsets.update(candidate_itemsets)\n",
        "\n",
        "        # Update current itemsets for the next iteration\n",
        "        current_itemsets = list(candidate_itemsets.keys())\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "\n",
        "def main(path, name, k, min_support):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize File class to load data\n",
        "    file_handler = File(path, name)\n",
        "\n",
        "    # Load data\n",
        "    file_handler.load_data()\n",
        "\n",
        "    # Get the list of documents as transactions\n",
        "    documents = [file_handler.get_words_by_docID(doc_id) for doc_id in file_handler.get_unique_docs()]\n",
        "\n",
        "    # Apply Apriori algorithm to find frequent itemsets of size k\n",
        "    print(\"Running Apriori algorithm...\")\n",
        "    frequent_itemsets = apriori(documents, min_support, k)\n",
        "\n",
        "    # Print the results for frequent itemsets of size k\n",
        "    print(\"\\nFrequent Itemsets of size\", k, \":\")\n",
        "    if frequent_itemsets:\n",
        "        for itemset, count in tqdm(frequent_itemsets.items(), desc=\"Printing itemsets\", colour=\"blue\"):\n",
        "            print(f\"Itemset: {set(itemset)}, Support: {count}\")\n",
        "    else:\n",
        "        print(f\"No frequent itemsets of size {k} found.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    dataset_name = input(\"Enter dataset name (e.g., enron): \")\n",
        "    k = int(input(\"Enter k (size of itemsets): \"))\n",
        "    min_support = int(input(\"Enter min_support (minimum frequency threshold): \"))\n",
        "    path = r\"/content/drive/MyDrive/bag+of+words\"  # Adjust path if needed\n",
        "    main(path, dataset_name, k, min_support)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XaXiJKAJVmr",
        "outputId": "b3f81122-5594-4803-c72a-dcec8b297964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter dataset name (e.g., enron): nips\n",
            "Enter k (size of itemsets): 2\n",
            "Enter min_support (minimum frequency threshold): 800\n",
            "Reading vocabulary file...\n",
            "Vocabulary file loaded. Time taken: 0.01 seconds.\n",
            "Reading docword file (metadata)...\n",
            "Docword file metadata loaded. Time taken: 0.01 seconds.\n",
            "Reading docword file (data)...\n",
            "\n",
            "\n",
            "Docword file data loaded. Time taken: 0.45 seconds.\n",
            "Running Apriori algorithm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 1-itemsets: 100%|\u001b[32m██████████\u001b[0m| 1500/1500 [00:00<00:00, 2064.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating 2-itemsets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 2-itemsets candidates: 100%|\u001b[33m██████████\u001b[0m| 75/75 [01:53<00:00,  1.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Frequent Itemsets of size 2 :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Printing itemsets: 100%|\u001b[34m██████████\u001b[0m| 724/724 [00:00<00:00, 108755.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Itemset: {316, 39}, Support: 1071\n",
            "Itemset: {428, 39}, Support: 893\n",
            "Itemset: {532, 39}, Support: 878\n",
            "Itemset: {540, 39}, Support: 976\n",
            "Itemset: {1482, 39}, Support: 1128\n",
            "Itemset: {2056, 39}, Support: 866\n",
            "Itemset: {2087, 39}, Support: 838\n",
            "Itemset: {2175, 39}, Support: 811\n",
            "Itemset: {2574, 39}, Support: 1128\n",
            "Itemset: {2676, 39}, Support: 823\n",
            "Itemset: {3603, 39}, Support: 967\n",
            "Itemset: {3750, 39}, Support: 810\n",
            "Itemset: {4146, 39}, Support: 1055\n",
            "Itemset: {4180, 39}, Support: 858\n",
            "Itemset: {4270, 39}, Support: 1355\n",
            "Itemset: {4372, 39}, Support: 920\n",
            "Itemset: {4810, 39}, Support: 835\n",
            "Itemset: {5399, 39}, Support: 1192\n",
            "Itemset: {5554, 39}, Support: 1291\n",
            "Itemset: {6056, 39}, Support: 1077\n",
            "Itemset: {6120, 39}, Support: 1132\n",
            "Itemset: {6245, 39}, Support: 923\n",
            "Itemset: {7011, 39}, Support: 1242\n",
            "Itemset: {7358, 39}, Support: 1277\n",
            "Itemset: {7365, 39}, Support: 1343\n",
            "Itemset: {7484, 39}, Support: 905\n",
            "Itemset: {7579, 39}, Support: 1276\n",
            "Itemset: {7623, 39}, Support: 823\n",
            "Itemset: {7787, 39}, Support: 1092\n",
            "Itemset: {7874, 39}, Support: 1052\n",
            "Itemset: {7949, 39}, Support: 1102\n",
            "Itemset: {7969, 39}, Support: 1057\n",
            "Itemset: {8051, 39}, Support: 877\n",
            "Itemset: {8140, 39}, Support: 920\n",
            "Itemset: {8370, 39}, Support: 1073\n",
            "Itemset: {8628, 39}, Support: 1187\n",
            "Itemset: {8637, 39}, Support: 858\n",
            "Itemset: {8940, 39}, Support: 807\n",
            "Itemset: {9010, 39}, Support: 810\n",
            "Itemset: {9134, 39}, Support: 1411\n",
            "Itemset: {9303, 39}, Support: 851\n",
            "Itemset: {9336, 39}, Support: 933\n",
            "Itemset: {9402, 39}, Support: 1389\n",
            "Itemset: {9840, 39}, Support: 893\n",
            "Itemset: {10003, 39}, Support: 1324\n",
            "Itemset: {10174, 39}, Support: 987\n",
            "Itemset: {10181, 39}, Support: 1001\n",
            "Itemset: {10305, 39}, Support: 1038\n",
            "Itemset: {10416, 39}, Support: 982\n",
            "Itemset: {11020, 39}, Support: 1329\n",
            "Itemset: {11161, 39}, Support: 983\n",
            "Itemset: {11297, 39}, Support: 923\n",
            "Itemset: {11418, 39}, Support: 911\n",
            "Itemset: {11719, 39}, Support: 895\n",
            "Itemset: {11735, 39}, Support: 1014\n",
            "Itemset: {11901, 39}, Support: 890\n",
            "Itemset: {12153, 39}, Support: 941\n",
            "Itemset: {5344, 39}, Support: 1182\n",
            "Itemset: {5934, 39}, Support: 813\n",
            "Itemset: {6742, 39}, Support: 1009\n",
            "Itemset: {6836, 39}, Support: 1024\n",
            "Itemset: {7538, 39}, Support: 852\n",
            "Itemset: {8548, 39}, Support: 890\n",
            "Itemset: {8561, 39}, Support: 862\n",
            "Itemset: {10217, 39}, Support: 1026\n",
            "Itemset: {11857, 39}, Support: 1050\n",
            "Itemset: {12383, 39}, Support: 801\n",
            "Itemset: {2051, 39}, Support: 919\n",
            "Itemset: {8640, 39}, Support: 1068\n",
            "Itemset: {9879, 39}, Support: 813\n",
            "Itemset: {10620, 39}, Support: 840\n",
            "Itemset: {316, 540}, Support: 807\n",
            "Itemset: {1482, 316}, Support: 847\n",
            "Itemset: {316, 2574}, Support: 885\n",
            "Itemset: {316, 4270}, Support: 1014\n",
            "Itemset: {316, 5399}, Support: 868\n",
            "Itemset: {5554, 316}, Support: 960\n",
            "Itemset: {6056, 316}, Support: 812\n",
            "Itemset: {6120, 316}, Support: 931\n",
            "Itemset: {7011, 316}, Support: 918\n",
            "Itemset: {316, 7358}, Support: 955\n",
            "Itemset: {316, 7365}, Support: 1002\n",
            "Itemset: {7579, 316}, Support: 968\n",
            "Itemset: {7787, 316}, Support: 824\n",
            "Itemset: {316, 7949}, Support: 846\n",
            "Itemset: {7969, 316}, Support: 816\n",
            "Itemset: {8370, 316}, Support: 823\n",
            "Itemset: {8628, 316}, Support: 973\n",
            "Itemset: {316, 9134}, Support: 1050\n",
            "Itemset: {9402, 316}, Support: 1039\n",
            "Itemset: {10003, 316}, Support: 1021\n",
            "Itemset: {316, 11020}, Support: 989\n",
            "Itemset: {5344, 316}, Support: 906\n",
            "Itemset: {6836, 316}, Support: 841\n",
            "Itemset: {11857, 316}, Support: 800\n",
            "Itemset: {8640, 316}, Support: 820\n",
            "Itemset: {428, 4270}, Support: 843\n",
            "Itemset: {5554, 428}, Support: 804\n",
            "Itemset: {428, 7365}, Support: 831\n",
            "Itemset: {7579, 428}, Support: 807\n",
            "Itemset: {428, 9134}, Support: 874\n",
            "Itemset: {9402, 428}, Support: 873\n",
            "Itemset: {10003, 428}, Support: 829\n",
            "Itemset: {428, 11020}, Support: 813\n",
            "Itemset: {532, 4270}, Support: 832\n",
            "Itemset: {5554, 532}, Support: 802\n",
            "Itemset: {532, 7365}, Support: 833\n",
            "Itemset: {532, 9134}, Support: 861\n",
            "Itemset: {9402, 532}, Support: 853\n",
            "Itemset: {10003, 532}, Support: 835\n",
            "Itemset: {532, 11020}, Support: 812\n",
            "Itemset: {540, 2574}, Support: 807\n",
            "Itemset: {540, 4270}, Support: 924\n",
            "Itemset: {5554, 540}, Support: 872\n",
            "Itemset: {6120, 540}, Support: 815\n",
            "Itemset: {7011, 540}, Support: 853\n",
            "Itemset: {540, 7358}, Support: 876\n",
            "Itemset: {540, 7365}, Support: 919\n",
            "Itemset: {7579, 540}, Support: 884\n",
            "Itemset: {8628, 540}, Support: 869\n",
            "Itemset: {540, 9134}, Support: 954\n",
            "Itemset: {9402, 540}, Support: 946\n",
            "Itemset: {10003, 540}, Support: 933\n",
            "Itemset: {540, 11020}, Support: 901\n",
            "Itemset: {5344, 540}, Support: 827\n",
            "Itemset: {1482, 2574}, Support: 879\n",
            "Itemset: {1482, 4146}, Support: 843\n",
            "Itemset: {1482, 4270}, Support: 1071\n",
            "Itemset: {1482, 5399}, Support: 919\n",
            "Itemset: {5554, 1482}, Support: 1012\n",
            "Itemset: {6056, 1482}, Support: 843\n",
            "Itemset: {6120, 1482}, Support: 908\n",
            "Itemset: {1482, 7011}, Support: 967\n",
            "Itemset: {1482, 7358}, Support: 995\n",
            "Itemset: {1482, 7365}, Support: 1051\n",
            "Itemset: {1482, 7579}, Support: 1003\n",
            "Itemset: {1482, 7787}, Support: 875\n",
            "Itemset: {7874, 1482}, Support: 809\n",
            "Itemset: {1482, 7949}, Support: 882\n",
            "Itemset: {7969, 1482}, Support: 843\n",
            "Itemset: {8370, 1482}, Support: 861\n",
            "Itemset: {1482, 8628}, Support: 951\n",
            "Itemset: {1482, 9134}, Support: 1102\n",
            "Itemset: {9402, 1482}, Support: 1091\n",
            "Itemset: {1482, 10003}, Support: 1045\n",
            "Itemset: {1482, 10181}, Support: 807\n",
            "Itemset: {10305, 1482}, Support: 809\n",
            "Itemset: {1482, 11020}, Support: 1026\n",
            "Itemset: {11161, 1482}, Support: 802\n",
            "Itemset: {5344, 1482}, Support: 929\n",
            "Itemset: {1482, 6742}, Support: 812\n",
            "Itemset: {1482, 6836}, Support: 814\n",
            "Itemset: {11857, 1482}, Support: 838\n",
            "Itemset: {8640, 1482}, Support: 823\n",
            "Itemset: {2056, 4270}, Support: 817\n",
            "Itemset: {2056, 7365}, Support: 810\n",
            "Itemset: {2056, 9134}, Support: 858\n",
            "Itemset: {2056, 9402}, Support: 840\n",
            "Itemset: {2056, 10003}, Support: 815\n",
            "Itemset: {2056, 11020}, Support: 803\n",
            "Itemset: {9134, 2087}, Support: 817\n",
            "Itemset: {9402, 2087}, Support: 810\n",
            "Itemset: {3603, 2574}, Support: 813\n",
            "Itemset: {4146, 2574}, Support: 825\n",
            "Itemset: {4270, 2574}, Support: 1061\n",
            "Itemset: {2574, 5399}, Support: 936\n",
            "Itemset: {5554, 2574}, Support: 1013\n",
            "Itemset: {6056, 2574}, Support: 865\n",
            "Itemset: {6120, 2574}, Support: 914\n",
            "Itemset: {7011, 2574}, Support: 991\n",
            "Itemset: {7358, 2574}, Support: 1007\n",
            "Itemset: {7365, 2574}, Support: 1062\n",
            "Itemset: {7579, 2574}, Support: 1006\n",
            "Itemset: {7787, 2574}, Support: 854\n",
            "Itemset: {7874, 2574}, Support: 846\n",
            "Itemset: {7949, 2574}, Support: 866\n",
            "Itemset: {7969, 2574}, Support: 848\n",
            "Itemset: {8370, 2574}, Support: 853\n",
            "Itemset: {8628, 2574}, Support: 957\n",
            "Itemset: {9134, 2574}, Support: 1105\n",
            "Itemset: {9402, 2574}, Support: 1089\n",
            "Itemset: {10003, 2574}, Support: 1061\n",
            "Itemset: {10305, 2574}, Support: 831\n",
            "Itemset: {11020, 2574}, Support: 1035\n",
            "Itemset: {5344, 2574}, Support: 945\n",
            "Itemset: {6742, 2574}, Support: 807\n",
            "Itemset: {6836, 2574}, Support: 834\n",
            "Itemset: {10217, 2574}, Support: 808\n",
            "Itemset: {11857, 2574}, Support: 833\n",
            "Itemset: {8640, 2574}, Support: 854\n",
            "Itemset: {2676, 9134}, Support: 806\n",
            "Itemset: {3603, 4270}, Support: 919\n",
            "Itemset: {3603, 5399}, Support: 825\n",
            "Itemset: {5554, 3603}, Support: 863\n",
            "Itemset: {6120, 3603}, Support: 839\n",
            "Itemset: {3603, 7011}, Support: 823\n",
            "Itemset: {3603, 7358}, Support: 881\n",
            "Itemset: {3603, 7365}, Support: 913\n",
            "Itemset: {3603, 7579}, Support: 887\n",
            "Itemset: {3603, 8628}, Support: 856\n",
            "Itemset: {3603, 9134}, Support: 946\n",
            "Itemset: {9402, 3603}, Support: 939\n",
            "Itemset: {10003, 3603}, Support: 922\n",
            "Itemset: {3603, 11020}, Support: 903\n",
            "Itemset: {5344, 3603}, Support: 817\n",
            "Itemset: {4146, 4270}, Support: 1007\n",
            "Itemset: {4146, 5399}, Support: 875\n",
            "Itemset: {5554, 4146}, Support: 946\n",
            "Itemset: {6120, 4146}, Support: 851\n",
            "Itemset: {4146, 7011}, Support: 921\n",
            "Itemset: {4146, 7358}, Support: 948\n",
            "Itemset: {4146, 7365}, Support: 989\n",
            "Itemset: {4146, 7579}, Support: 927\n",
            "Itemset: {4146, 7787}, Support: 807\n",
            "Itemset: {4146, 7949}, Support: 826\n",
            "Itemset: {8370, 4146}, Support: 801\n",
            "Itemset: {4146, 8628}, Support: 883\n",
            "Itemset: {4146, 9134}, Support: 1030\n",
            "Itemset: {9402, 4146}, Support: 1019\n",
            "Itemset: {4146, 10003}, Support: 974\n",
            "Itemset: {4146, 11020}, Support: 974\n",
            "Itemset: {5344, 4146}, Support: 871\n",
            "Itemset: {4180, 4270}, Support: 811\n",
            "Itemset: {4180, 7365}, Support: 802\n",
            "Itemset: {4180, 9134}, Support: 838\n",
            "Itemset: {9402, 4180}, Support: 835\n",
            "Itemset: {4372, 4270}, Support: 881\n",
            "Itemset: {4270, 5399}, Support: 1127\n",
            "Itemset: {5554, 4270}, Support: 1207\n",
            "Itemset: {6056, 4270}, Support: 1012\n",
            "Itemset: {6120, 4270}, Support: 1079\n",
            "Itemset: {6245, 4270}, Support: 890\n",
            "Itemset: {7011, 4270}, Support: 1184\n",
            "Itemset: {7358, 4270}, Support: 1211\n",
            "Itemset: {7365, 4270}, Support: 1274\n",
            "Itemset: {7484, 4270}, Support: 856\n",
            "Itemset: {7579, 4270}, Support: 1205\n",
            "Itemset: {7787, 4270}, Support: 1041\n",
            "Itemset: {7874, 4270}, Support: 993\n",
            "Itemset: {7949, 4270}, Support: 1041\n",
            "Itemset: {7969, 4270}, Support: 1024\n",
            "Itemset: {8051, 4270}, Support: 820\n",
            "Itemset: {8140, 4270}, Support: 859\n",
            "Itemset: {8370, 4270}, Support: 1030\n",
            "Itemset: {8628, 4270}, Support: 1128\n",
            "Itemset: {8637, 4270}, Support: 803\n",
            "Itemset: {9134, 4270}, Support: 1327\n",
            "Itemset: {9336, 4270}, Support: 869\n",
            "Itemset: {9402, 4270}, Support: 1309\n",
            "Itemset: {9840, 4270}, Support: 838\n",
            "Itemset: {10003, 4270}, Support: 1255\n",
            "Itemset: {10174, 4270}, Support: 932\n",
            "Itemset: {10181, 4270}, Support: 957\n",
            "Itemset: {10305, 4270}, Support: 978\n",
            "Itemset: {10416, 4270}, Support: 935\n",
            "Itemset: {11020, 4270}, Support: 1252\n",
            "Itemset: {11161, 4270}, Support: 956\n",
            "Itemset: {11297, 4270}, Support: 878\n",
            "Itemset: {11418, 4270}, Support: 865\n",
            "Itemset: {4270, 11719}, Support: 846\n",
            "Itemset: {4270, 11735}, Support: 965\n",
            "Itemset: {11901, 4270}, Support: 843\n",
            "Itemset: {12153, 4270}, Support: 899\n",
            "Itemset: {5344, 4270}, Support: 1113\n",
            "Itemset: {6742, 4270}, Support: 960\n",
            "Itemset: {6836, 4270}, Support: 982\n",
            "Itemset: {7538, 4270}, Support: 810\n",
            "Itemset: {8548, 4270}, Support: 836\n",
            "Itemset: {8561, 4270}, Support: 824\n",
            "Itemset: {10217, 4270}, Support: 970\n",
            "Itemset: {11857, 4270}, Support: 995\n",
            "Itemset: {2051, 4270}, Support: 877\n",
            "Itemset: {8640, 4270}, Support: 1003\n",
            "Itemset: {5554, 4372}, Support: 813\n",
            "Itemset: {4372, 7358}, Support: 829\n",
            "Itemset: {4372, 7365}, Support: 859\n",
            "Itemset: {7579, 4372}, Support: 828\n",
            "Itemset: {4372, 9134}, Support: 899\n",
            "Itemset: {9402, 4372}, Support: 892\n",
            "Itemset: {10003, 4372}, Support: 854\n",
            "Itemset: {4372, 11020}, Support: 841\n",
            "Itemset: {4810, 9134}, Support: 816\n",
            "Itemset: {9402, 4810}, Support: 807\n",
            "Itemset: {5554, 5399}, Support: 1059\n",
            "Itemset: {6056, 5399}, Support: 887\n",
            "Itemset: {6120, 5399}, Support: 952\n",
            "Itemset: {7011, 5399}, Support: 1029\n",
            "Itemset: {7358, 5399}, Support: 1122\n",
            "Itemset: {7365, 5399}, Support: 1144\n",
            "Itemset: {7579, 5399}, Support: 1053\n",
            "Itemset: {7787, 5399}, Support: 913\n",
            "Itemset: {7874, 5399}, Support: 1010\n",
            "Itemset: {7949, 5399}, Support: 898\n",
            "Itemset: {7969, 5399}, Support: 867\n",
            "Itemset: {8370, 5399}, Support: 877\n",
            "Itemset: {8628, 5399}, Support: 969\n",
            "Itemset: {9134, 5399}, Support: 1164\n",
            "Itemset: {9402, 5399}, Support: 1153\n",
            "Itemset: {10003, 5399}, Support: 1101\n",
            "Itemset: {10174, 5399}, Support: 818\n",
            "Itemset: {10181, 5399}, Support: 817\n",
            "Itemset: {10305, 5399}, Support: 868\n",
            "Itemset: {11020, 5399}, Support: 1125\n",
            "Itemset: {11161, 5399}, Support: 806\n",
            "Itemset: {11418, 5399}, Support: 816\n",
            "Itemset: {11719, 5399}, Support: 807\n",
            "Itemset: {11735, 5399}, Support: 837\n",
            "Itemset: {12153, 5399}, Support: 853\n",
            "Itemset: {5344, 5399}, Support: 984\n",
            "Itemset: {6742, 5399}, Support: 815\n",
            "Itemset: {6836, 5399}, Support: 828\n",
            "Itemset: {10217, 5399}, Support: 876\n",
            "Itemset: {11857, 5399}, Support: 869\n",
            "Itemset: {8640, 5399}, Support: 924\n",
            "Itemset: {6056, 5554}, Support: 965\n",
            "Itemset: {6120, 5554}, Support: 1008\n",
            "Itemset: {5554, 6245}, Support: 829\n",
            "Itemset: {5554, 7011}, Support: 1110\n",
            "Itemset: {5554, 7358}, Support: 1145\n",
            "Itemset: {5554, 7365}, Support: 1202\n",
            "Itemset: {5554, 7484}, Support: 804\n",
            "Itemset: {5554, 7579}, Support: 1139\n",
            "Itemset: {5554, 7787}, Support: 974\n",
            "Itemset: {7874, 5554}, Support: 940\n",
            "Itemset: {5554, 7949}, Support: 987\n",
            "Itemset: {7969, 5554}, Support: 948\n",
            "Itemset: {5554, 8140}, Support: 827\n",
            "Itemset: {8370, 5554}, Support: 962\n",
            "Itemset: {5554, 8628}, Support: 1065\n",
            "Itemset: {5554, 9134}, Support: 1256\n",
            "Itemset: {9336, 5554}, Support: 843\n",
            "Itemset: {9402, 5554}, Support: 1242\n",
            "Itemset: {5554, 10003}, Support: 1186\n",
            "Itemset: {5554, 10174}, Support: 884\n",
            "Itemset: {5554, 10181}, Support: 889\n",
            "Itemset: {10305, 5554}, Support: 935\n",
            "Itemset: {10416, 5554}, Support: 871\n",
            "Itemset: {5554, 11020}, Support: 1182\n",
            "Itemset: {11161, 5554}, Support: 875\n",
            "Itemset: {11297, 5554}, Support: 815\n",
            "Itemset: {5554, 11418}, Support: 818\n",
            "Itemset: {5554, 11719}, Support: 805\n",
            "Itemset: {5554, 11735}, Support: 904\n",
            "Itemset: {5554, 11901}, Support: 804\n",
            "Itemset: {12153, 5554}, Support: 837\n",
            "Itemset: {5344, 5554}, Support: 1057\n",
            "Itemset: {5554, 6742}, Support: 907\n",
            "Itemset: {5554, 6836}, Support: 933\n",
            "Itemset: {10217, 5554}, Support: 912\n",
            "Itemset: {11857, 5554}, Support: 938\n",
            "Itemset: {5554, 2051}, Support: 819\n",
            "Itemset: {8640, 5554}, Support: 956\n",
            "Itemset: {6056, 6120}, Support: 857\n",
            "Itemset: {6056, 7011}, Support: 920\n",
            "Itemset: {6056, 7358}, Support: 957\n",
            "Itemset: {6056, 7365}, Support: 1004\n",
            "Itemset: {6056, 7579}, Support: 974\n",
            "Itemset: {6056, 7787}, Support: 824\n",
            "Itemset: {6056, 7949}, Support: 812\n",
            "Itemset: {6056, 7969}, Support: 807\n",
            "Itemset: {6056, 8370}, Support: 823\n",
            "Itemset: {6056, 8628}, Support: 902\n",
            "Itemset: {6056, 9134}, Support: 1046\n",
            "Itemset: {6056, 9402}, Support: 1047\n",
            "Itemset: {6056, 10003}, Support: 992\n",
            "Itemset: {6056, 10305}, Support: 834\n",
            "Itemset: {6056, 11020}, Support: 990\n",
            "Itemset: {6056, 5344}, Support: 873\n",
            "Itemset: {6056, 11857}, Support: 806\n",
            "Itemset: {6120, 7011}, Support: 986\n",
            "Itemset: {6120, 7358}, Support: 1027\n",
            "Itemset: {6120, 7365}, Support: 1069\n",
            "Itemset: {6120, 7579}, Support: 1031\n",
            "Itemset: {6120, 7787}, Support: 861\n",
            "Itemset: {6120, 7874}, Support: 847\n",
            "Itemset: {6120, 7949}, Support: 884\n",
            "Itemset: {6120, 7969}, Support: 861\n",
            "Itemset: {6120, 8370}, Support: 851\n",
            "Itemset: {6120, 8628}, Support: 999\n",
            "Itemset: {6120, 9134}, Support: 1113\n",
            "Itemset: {6120, 9402}, Support: 1095\n",
            "Itemset: {6120, 10003}, Support: 1081\n",
            "Itemset: {6120, 10181}, Support: 803\n",
            "Itemset: {6120, 10305}, Support: 814\n",
            "Itemset: {6120, 10416}, Support: 822\n",
            "Itemset: {6120, 11020}, Support: 1044\n",
            "Itemset: {6120, 11161}, Support: 801\n",
            "Itemset: {6120, 11418}, Support: 848\n",
            "Itemset: {6120, 11735}, Support: 817\n",
            "Itemset: {6120, 12153}, Support: 827\n",
            "Itemset: {6120, 5344}, Support: 960\n",
            "Itemset: {6120, 6742}, Support: 802\n",
            "Itemset: {6120, 6836}, Support: 853\n",
            "Itemset: {6120, 11857}, Support: 852\n",
            "Itemset: {6120, 8640}, Support: 846\n",
            "Itemset: {7011, 6245}, Support: 819\n",
            "Itemset: {6245, 7358}, Support: 835\n",
            "Itemset: {7365, 6245}, Support: 873\n",
            "Itemset: {7579, 6245}, Support: 831\n",
            "Itemset: {6245, 9134}, Support: 906\n",
            "Itemset: {9402, 6245}, Support: 894\n",
            "Itemset: {10003, 6245}, Support: 862\n",
            "Itemset: {11020, 6245}, Support: 843\n",
            "Itemset: {7011, 7358}, Support: 1108\n",
            "Itemset: {7011, 7365}, Support: 1163\n",
            "Itemset: {7011, 7579}, Support: 1098\n",
            "Itemset: {7011, 7787}, Support: 949\n",
            "Itemset: {7874, 7011}, Support: 907\n",
            "Itemset: {7011, 7949}, Support: 959\n",
            "Itemset: {7969, 7011}, Support: 935\n",
            "Itemset: {8370, 7011}, Support: 924\n",
            "Itemset: {7011, 8628}, Support: 1021\n",
            "Itemset: {7011, 9134}, Support: 1215\n",
            "Itemset: {9336, 7011}, Support: 809\n",
            "Itemset: {9402, 7011}, Support: 1201\n",
            "Itemset: {10003, 7011}, Support: 1147\n",
            "Itemset: {7011, 10174}, Support: 863\n",
            "Itemset: {7011, 10181}, Support: 873\n",
            "Itemset: {10305, 7011}, Support: 885\n",
            "Itemset: {10416, 7011}, Support: 848\n",
            "Itemset: {7011, 11020}, Support: 1158\n",
            "Itemset: {11161, 7011}, Support: 866\n",
            "Itemset: {11297, 7011}, Support: 802\n",
            "Itemset: {7011, 11735}, Support: 883\n",
            "Itemset: {12153, 7011}, Support: 813\n",
            "Itemset: {5344, 7011}, Support: 1030\n",
            "Itemset: {7011, 6742}, Support: 883\n",
            "Itemset: {7011, 6836}, Support: 898\n",
            "Itemset: {10217, 7011}, Support: 888\n",
            "Itemset: {11857, 7011}, Support: 902\n",
            "Itemset: {2051, 7011}, Support: 803\n",
            "Itemset: {8640, 7011}, Support: 928\n",
            "Itemset: {7365, 7358}, Support: 1255\n",
            "Itemset: {7579, 7358}, Support: 1133\n",
            "Itemset: {7787, 7358}, Support: 970\n",
            "Itemset: {7874, 7358}, Support: 985\n",
            "Itemset: {7949, 7358}, Support: 976\n",
            "Itemset: {7969, 7358}, Support: 949\n",
            "Itemset: {8051, 7358}, Support: 813\n",
            "Itemset: {8140, 7358}, Support: 830\n",
            "Itemset: {8370, 7358}, Support: 954\n",
            "Itemset: {8628, 7358}, Support: 1060\n",
            "Itemset: {9134, 7358}, Support: 1250\n",
            "Itemset: {9336, 7358}, Support: 836\n",
            "Itemset: {9402, 7358}, Support: 1247\n",
            "Itemset: {10003, 7358}, Support: 1181\n",
            "Itemset: {10174, 7358}, Support: 874\n",
            "Itemset: {10181, 7358}, Support: 893\n",
            "Itemset: {10305, 7358}, Support: 922\n",
            "Itemset: {10416, 7358}, Support: 860\n",
            "Itemset: {11020, 7358}, Support: 1197\n",
            "Itemset: {11161, 7358}, Support: 879\n",
            "Itemset: {11297, 7358}, Support: 835\n",
            "Itemset: {11418, 7358}, Support: 858\n",
            "Itemset: {7358, 11719}, Support: 847\n",
            "Itemset: {7358, 11735}, Support: 911\n",
            "Itemset: {11901, 7358}, Support: 806\n",
            "Itemset: {12153, 7358}, Support: 908\n",
            "Itemset: {5344, 7358}, Support: 1054\n",
            "Itemset: {6742, 7358}, Support: 898\n",
            "Itemset: {6836, 7358}, Support: 914\n",
            "Itemset: {10217, 7358}, Support: 920\n",
            "Itemset: {11857, 7358}, Support: 925\n",
            "Itemset: {2051, 7358}, Support: 831\n",
            "Itemset: {8640, 7358}, Support: 970\n",
            "Itemset: {7484, 7365}, Support: 830\n",
            "Itemset: {7579, 7365}, Support: 1192\n",
            "Itemset: {7787, 7365}, Support: 1031\n",
            "Itemset: {7874, 7365}, Support: 1012\n",
            "Itemset: {7949, 7365}, Support: 1027\n",
            "Itemset: {7969, 7365}, Support: 992\n",
            "Itemset: {8051, 7365}, Support: 838\n",
            "Itemset: {8140, 7365}, Support: 869\n",
            "Itemset: {8370, 7365}, Support: 999\n",
            "Itemset: {8628, 7365}, Support: 1114\n",
            "Itemset: {7365, 8637}, Support: 806\n",
            "Itemset: {7365, 9134}, Support: 1314\n",
            "Itemset: {9336, 7365}, Support: 873\n",
            "Itemset: {9402, 7365}, Support: 1296\n",
            "Itemset: {9840, 7365}, Support: 830\n",
            "Itemset: {10003, 7365}, Support: 1243\n",
            "Itemset: {7365, 10174}, Support: 914\n",
            "Itemset: {10181, 7365}, Support: 942\n",
            "Itemset: {10305, 7365}, Support: 969\n",
            "Itemset: {10416, 7365}, Support: 910\n",
            "Itemset: {11020, 7365}, Support: 1264\n",
            "Itemset: {11161, 7365}, Support: 916\n",
            "Itemset: {11297, 7365}, Support: 864\n",
            "Itemset: {11418, 7365}, Support: 873\n",
            "Itemset: {7365, 11719}, Support: 858\n",
            "Itemset: {7365, 11735}, Support: 955\n",
            "Itemset: {11901, 7365}, Support: 837\n",
            "Itemset: {12153, 7365}, Support: 912\n",
            "Itemset: {5344, 7365}, Support: 1121\n",
            "Itemset: {7365, 6742}, Support: 946\n",
            "Itemset: {6836, 7365}, Support: 961\n",
            "Itemset: {8548, 7365}, Support: 824\n",
            "Itemset: {8561, 7365}, Support: 813\n",
            "Itemset: {10217, 7365}, Support: 962\n",
            "Itemset: {11857, 7365}, Support: 976\n",
            "Itemset: {2051, 7365}, Support: 887\n",
            "Itemset: {8640, 7365}, Support: 1025\n",
            "Itemset: {7579, 7484}, Support: 806\n",
            "Itemset: {7484, 9134}, Support: 880\n",
            "Itemset: {9402, 7484}, Support: 877\n",
            "Itemset: {10003, 7484}, Support: 831\n",
            "Itemset: {7484, 11020}, Support: 826\n",
            "Itemset: {7579, 7787}, Support: 984\n",
            "Itemset: {7874, 7579}, Support: 934\n",
            "Itemset: {7579, 7949}, Support: 987\n",
            "Itemset: {7969, 7579}, Support: 954\n",
            "Itemset: {7579, 8140}, Support: 845\n",
            "Itemset: {8370, 7579}, Support: 961\n",
            "Itemset: {7579, 8628}, Support: 1075\n",
            "Itemset: {7579, 9134}, Support: 1248\n",
            "Itemset: {9336, 7579}, Support: 834\n",
            "Itemset: {9402, 7579}, Support: 1236\n",
            "Itemset: {10003, 7579}, Support: 1195\n",
            "Itemset: {7579, 10174}, Support: 875\n",
            "Itemset: {7579, 10181}, Support: 898\n",
            "Itemset: {10305, 7579}, Support: 941\n",
            "Itemset: {10416, 7579}, Support: 879\n",
            "Itemset: {7579, 11020}, Support: 1181\n",
            "Itemset: {11161, 7579}, Support: 884\n",
            "Itemset: {11297, 7579}, Support: 807\n",
            "Itemset: {11418, 7579}, Support: 846\n",
            "Itemset: {7579, 11719}, Support: 807\n",
            "Itemset: {7579, 11735}, Support: 906\n",
            "Itemset: {7579, 11901}, Support: 806\n",
            "Itemset: {12153, 7579}, Support: 848\n",
            "Itemset: {5344, 7579}, Support: 1058\n",
            "Itemset: {7579, 6742}, Support: 917\n",
            "Itemset: {7579, 6836}, Support: 936\n",
            "Itemset: {10217, 7579}, Support: 910\n",
            "Itemset: {11857, 7579}, Support: 947\n",
            "Itemset: {2051, 7579}, Support: 822\n",
            "Itemset: {8640, 7579}, Support: 950\n",
            "Itemset: {9134, 7623}, Support: 801\n",
            "Itemset: {9402, 7623}, Support: 809\n",
            "Itemset: {7874, 7787}, Support: 802\n",
            "Itemset: {7787, 7949}, Support: 830\n",
            "Itemset: {7969, 7787}, Support: 819\n",
            "Itemset: {8370, 7787}, Support: 816\n",
            "Itemset: {7787, 8628}, Support: 911\n",
            "Itemset: {7787, 9134}, Support: 1065\n",
            "Itemset: {9402, 7787}, Support: 1055\n",
            "Itemset: {10003, 7787}, Support: 1012\n",
            "Itemset: {10305, 7787}, Support: 801\n",
            "Itemset: {7787, 11020}, Support: 1015\n",
            "Itemset: {5344, 7787}, Support: 897\n",
            "Itemset: {8640, 7787}, Support: 805\n",
            "Itemset: {7874, 8628}, Support: 869\n",
            "Itemset: {7874, 9134}, Support: 1027\n",
            "Itemset: {9402, 7874}, Support: 1019\n",
            "Itemset: {7874, 10003}, Support: 981\n",
            "Itemset: {7874, 11020}, Support: 998\n",
            "Itemset: {5344, 7874}, Support: 870\n",
            "Itemset: {8640, 7874}, Support: 828\n",
            "Itemset: {7969, 7949}, Support: 814\n",
            "Itemset: {8370, 7949}, Support: 841\n",
            "Itemset: {8628, 7949}, Support: 943\n",
            "Itemset: {7949, 9134}, Support: 1072\n",
            "Itemset: {9402, 7949}, Support: 1058\n",
            "Itemset: {10003, 7949}, Support: 1017\n",
            "Itemset: {11020, 7949}, Support: 1009\n",
            "Itemset: {5344, 7949}, Support: 907\n",
            "Itemset: {6836, 7949}, Support: 812\n",
            "Itemset: {11857, 7949}, Support: 809\n",
            "Itemset: {8640, 7949}, Support: 806\n",
            "Itemset: {7969, 8370}, Support: 810\n",
            "Itemset: {7969, 8628}, Support: 884\n",
            "Itemset: {7969, 9134}, Support: 1038\n",
            "Itemset: {7969, 9402}, Support: 1018\n",
            "Itemset: {7969, 10003}, Support: 991\n",
            "Itemset: {7969, 11020}, Support: 974\n",
            "Itemset: {5344, 7969}, Support: 862\n",
            "Itemset: {11857, 7969}, Support: 805\n",
            "Itemset: {8051, 9134}, Support: 855\n",
            "Itemset: {9402, 8051}, Support: 851\n",
            "Itemset: {10003, 8051}, Support: 814\n",
            "Itemset: {8051, 11020}, Support: 824\n",
            "Itemset: {8628, 8140}, Support: 802\n",
            "Itemset: {8140, 9134}, Support: 904\n",
            "Itemset: {9402, 8140}, Support: 890\n",
            "Itemset: {10003, 8140}, Support: 876\n",
            "Itemset: {8140, 11020}, Support: 866\n",
            "Itemset: {8370, 8628}, Support: 916\n",
            "Itemset: {8370, 9134}, Support: 1048\n",
            "Itemset: {9402, 8370}, Support: 1033\n",
            "Itemset: {8370, 10003}, Support: 988\n",
            "Itemset: {8370, 11020}, Support: 988\n",
            "Itemset: {5344, 8370}, Support: 883\n",
            "Itemset: {11857, 8370}, Support: 808\n",
            "Itemset: {8628, 9134}, Support: 1165\n",
            "Itemset: {9402, 8628}, Support: 1147\n",
            "Itemset: {10003, 8628}, Support: 1128\n",
            "Itemset: {8628, 10174}, Support: 812\n",
            "Itemset: {8628, 10181}, Support: 852\n",
            "Itemset: {10305, 8628}, Support: 860\n",
            "Itemset: {10416, 8628}, Support: 860\n",
            "Itemset: {8628, 11020}, Support: 1097\n",
            "Itemset: {11161, 8628}, Support: 830\n",
            "Itemset: {11418, 8628}, Support: 834\n",
            "Itemset: {8628, 11735}, Support: 848\n",
            "Itemset: {12153, 8628}, Support: 824\n",
            "Itemset: {5344, 8628}, Support: 1004\n",
            "Itemset: {8628, 6742}, Support: 851\n",
            "Itemset: {6836, 8628}, Support: 906\n",
            "Itemset: {10217, 8628}, Support: 836\n",
            "Itemset: {11857, 8628}, Support: 878\n",
            "Itemset: {8640, 8628}, Support: 889\n",
            "Itemset: {8637, 9134}, Support: 838\n",
            "Itemset: {9402, 8637}, Support: 823\n",
            "Itemset: {10003, 8637}, Support: 801\n",
            "Itemset: {9134, 9303}, Support: 838\n",
            "Itemset: {9336, 9134}, Support: 914\n",
            "Itemset: {9402, 9134}, Support: 1358\n",
            "Itemset: {9840, 9134}, Support: 875\n",
            "Itemset: {10003, 9134}, Support: 1297\n",
            "Itemset: {10174, 9134}, Support: 965\n",
            "Itemset: {10181, 9134}, Support: 983\n",
            "Itemset: {10305, 9134}, Support: 1008\n",
            "Itemset: {10416, 9134}, Support: 962\n",
            "Itemset: {11020, 9134}, Support: 1299\n",
            "Itemset: {11161, 9134}, Support: 965\n",
            "Itemset: {11297, 9134}, Support: 900\n",
            "Itemset: {11418, 9134}, Support: 897\n",
            "Itemset: {9134, 11719}, Support: 874\n",
            "Itemset: {9134, 11735}, Support: 993\n",
            "Itemset: {11901, 9134}, Support: 870\n",
            "Itemset: {12153, 9134}, Support: 921\n",
            "Itemset: {5344, 9134}, Support: 1159\n",
            "Itemset: {6742, 9134}, Support: 984\n",
            "Itemset: {6836, 9134}, Support: 1005\n",
            "Itemset: {7538, 9134}, Support: 832\n",
            "Itemset: {8548, 9134}, Support: 863\n",
            "Itemset: {8561, 9134}, Support: 848\n",
            "Itemset: {10217, 9134}, Support: 1005\n",
            "Itemset: {11857, 9134}, Support: 1024\n",
            "Itemset: {2051, 9134}, Support: 899\n",
            "Itemset: {8640, 9134}, Support: 1049\n",
            "Itemset: {10620, 9134}, Support: 825\n",
            "Itemset: {9402, 9303}, Support: 815\n",
            "Itemset: {9336, 9402}, Support: 899\n",
            "Itemset: {9336, 10003}, Support: 860\n",
            "Itemset: {9336, 11020}, Support: 876\n",
            "Itemset: {9840, 9402}, Support: 863\n",
            "Itemset: {9402, 10003}, Support: 1280\n",
            "Itemset: {9402, 10174}, Support: 957\n",
            "Itemset: {9402, 10181}, Support: 962\n",
            "Itemset: {10305, 9402}, Support: 1008\n",
            "Itemset: {10416, 9402}, Support: 952\n",
            "Itemset: {9402, 11020}, Support: 1281\n",
            "Itemset: {11161, 9402}, Support: 956\n",
            "Itemset: {11297, 9402}, Support: 889\n",
            "Itemset: {9402, 11418}, Support: 893\n",
            "Itemset: {9402, 11719}, Support: 864\n",
            "Itemset: {9402, 11735}, Support: 976\n",
            "Itemset: {9402, 11901}, Support: 863\n",
            "Itemset: {12153, 9402}, Support: 916\n",
            "Itemset: {5344, 9402}, Support: 1141\n",
            "Itemset: {9402, 6742}, Support: 985\n",
            "Itemset: {9402, 6836}, Support: 998\n",
            "Itemset: {9402, 7538}, Support: 822\n",
            "Itemset: {9402, 8548}, Support: 863\n",
            "Itemset: {8561, 9402}, Support: 828\n",
            "Itemset: {10217, 9402}, Support: 994\n",
            "Itemset: {11857, 9402}, Support: 1011\n",
            "Itemset: {9402, 2051}, Support: 881\n",
            "Itemset: {8640, 9402}, Support: 1020\n",
            "Itemset: {9402, 10620}, Support: 802\n",
            "Itemset: {9840, 10003}, Support: 826\n",
            "Itemset: {9840, 11020}, Support: 826\n",
            "Itemset: {10003, 10174}, Support: 914\n",
            "Itemset: {10003, 10181}, Support: 932\n",
            "Itemset: {10305, 10003}, Support: 960\n",
            "Itemset: {10416, 10003}, Support: 910\n",
            "Itemset: {10003, 11020}, Support: 1224\n",
            "Itemset: {11161, 10003}, Support: 915\n",
            "Itemset: {11297, 10003}, Support: 843\n",
            "Itemset: {11418, 10003}, Support: 888\n",
            "Itemset: {10003, 11719}, Support: 830\n",
            "Itemset: {10003, 11735}, Support: 934\n",
            "Itemset: {10003, 11901}, Support: 846\n",
            "Itemset: {12153, 10003}, Support: 893\n",
            "Itemset: {5344, 10003}, Support: 1103\n",
            "Itemset: {10003, 6742}, Support: 935\n",
            "Itemset: {10003, 6836}, Support: 964\n",
            "Itemset: {10003, 8548}, Support: 820\n",
            "Itemset: {10217, 10003}, Support: 944\n",
            "Itemset: {11857, 10003}, Support: 978\n",
            "Itemset: {2051, 10003}, Support: 855\n",
            "Itemset: {8640, 10003}, Support: 984\n",
            "Itemset: {11020, 10174}, Support: 913\n",
            "Itemset: {5344, 10174}, Support: 817\n",
            "Itemset: {11020, 10181}, Support: 929\n",
            "Itemset: {5344, 10181}, Support: 841\n",
            "Itemset: {10305, 11020}, Support: 959\n",
            "Itemset: {5344, 10305}, Support: 845\n",
            "Itemset: {10416, 11020}, Support: 898\n",
            "Itemset: {10416, 5344}, Support: 805\n",
            "Itemset: {11161, 11020}, Support: 912\n",
            "Itemset: {11297, 11020}, Support: 853\n",
            "Itemset: {11418, 11020}, Support: 851\n",
            "Itemset: {11020, 11719}, Support: 845\n",
            "Itemset: {11020, 11735}, Support: 938\n",
            "Itemset: {11020, 11901}, Support: 808\n",
            "Itemset: {12153, 11020}, Support: 877\n",
            "Itemset: {5344, 11020}, Support: 1116\n",
            "Itemset: {11020, 6742}, Support: 929\n",
            "Itemset: {6836, 11020}, Support: 942\n",
            "Itemset: {8548, 11020}, Support: 828\n",
            "Itemset: {8561, 11020}, Support: 808\n",
            "Itemset: {10217, 11020}, Support: 949\n",
            "Itemset: {11857, 11020}, Support: 960\n",
            "Itemset: {2051, 11020}, Support: 850\n",
            "Itemset: {8640, 11020}, Support: 1033\n",
            "Itemset: {5344, 11161}, Support: 813\n",
            "Itemset: {5344, 11735}, Support: 843\n",
            "Itemset: {5344, 6742}, Support: 828\n",
            "Itemset: {5344, 6836}, Support: 862\n",
            "Itemset: {5344, 10217}, Support: 849\n",
            "Itemset: {5344, 11857}, Support: 849\n",
            "Itemset: {5344, 8640}, Support: 959\n",
            "Total Time taken: 117.17 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code run for k = 3, f = 900 with output on nips dataset"
      ],
      "metadata": {
        "id": "mgIptPQsdJCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "import time\n",
        "from pathlib import Path\n",
        "import os\n",
        "import errno\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "\n",
        "class File:\n",
        "    def __init__(self, path, name):\n",
        "        self.name = str(name)\n",
        "        self.vocabFile = \"vocab.\" + self.name + \".txt\"\n",
        "        self.vocabPath = Path(path) / self.vocabFile\n",
        "        self.docWordFile = \"docword.\" + self.name + \".txt.gz\"\n",
        "        self.docWordPath = Path(path) / self.docWordFile\n",
        "\n",
        "        if not self.vocabPath.exists() or self.vocabPath.is_dir():\n",
        "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.vocabPath)\n",
        "        if not self.docWordPath.exists() or self.docWordPath.is_dir():\n",
        "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.docWordPath)\n",
        "\n",
        "    def __read_csv_chunks(self, lines, path, **params):\n",
        "        chunks = []\n",
        "        for i, chunk in enumerate(pd.read_csv(path, **params)):\n",
        "            prog = min((i + 1) / lines * 100.0 * params[\"chunksize\"], 100)\n",
        "            print(f\"Data Load Progress : {prog:.2f} %.\", end=\"\\r\", flush=True)\n",
        "            chunks.append(chunk)\n",
        "        print(\"\\n\")\n",
        "        df = pd.concat(chunks, axis=0)\n",
        "        del chunks\n",
        "        return df\n",
        "\n",
        "    def load_data(self):\n",
        "        start_time = time.time()\n",
        "        print(\"Reading vocabulary file...\")\n",
        "        self.vocab = pd.read_csv(self.vocabPath, header=None, names=[\"word\"])\n",
        "        print(f\"Vocabulary file loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        self.vocab.index += 1\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"Reading docword file (metadata)...\")\n",
        "        tmp = pd.read_csv(self.docWordPath, compression='gzip', header=None, nrows=3)\n",
        "        self.docCount = tmp[0].values[0]\n",
        "        self.wordCount = tmp[0].values[1]\n",
        "        self.NNZ = tmp[0].values[2]\n",
        "        print(f\"Docword file metadata loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"Reading docword file (data)...\")\n",
        "        self.docWord = self.__read_csv_chunks(self.NNZ, self.docWordPath, compression='gzip', header=None, sep=' ',\n",
        "                                              quotechar='\"', on_bad_lines=\"skip\", skiprows=3, chunksize=10000,\n",
        "                                              names=[\"docID\", \"wordID\", \"count\"])\n",
        "        print(f\"Docword file data loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    def get_unique_docs(self):\n",
        "        try:\n",
        "            return self.docIDS\n",
        "        except AttributeError:\n",
        "            docIDS = self.docWord[\"docID\"].unique()\n",
        "            docIDS.sort()\n",
        "            self.docIDS = docIDS\n",
        "            return self.docIDS\n",
        "\n",
        "    def get_words_by_docID(self, _id):\n",
        "        try:\n",
        "            return self.docWordList[_id]\n",
        "        except AttributeError:\n",
        "            return self.docWord[self.docWord['docID'] == _id][\"wordID\"].tolist()\n",
        "\n",
        "\n",
        "# Optimized Apriori algorithm to find frequent itemsets of size k\n",
        "def apriori(dataset, min_support, k):\n",
        "    itemsets = defaultdict(int)\n",
        "\n",
        "    # Generate all itemsets of size 1 (frequent 1-itemsets)\n",
        "    for doc in tqdm(dataset, desc=\"Generating 1-itemsets\", colour=\"green\"):\n",
        "        for word in doc:\n",
        "            itemsets[frozenset([word])] += 1\n",
        "\n",
        "    # Prune infrequent itemsets\n",
        "    itemsets = {itemset: count for itemset, count in itemsets.items() if count >= min_support}\n",
        "\n",
        "    # Start generating k-itemsets\n",
        "    frequent_itemsets = defaultdict(int)\n",
        "    current_itemsets = list(itemsets.keys())\n",
        "\n",
        "    # We only need to keep itemsets of size k, so we stop the generation process once we reach k-itemsets.\n",
        "    for k_itemsets in range(2, k + 1):\n",
        "        print(f\"\\nGenerating {k_itemsets}-itemsets...\")\n",
        "\n",
        "        # Generate candidate itemsets from the previous frequent itemsets\n",
        "        candidate_itemsets = defaultdict(int)\n",
        "\n",
        "        # Join itemsets of size (k_itemsets - 1) to generate k_itemsets\n",
        "        for i in tqdm(range(len(current_itemsets)), desc=f\"Generating {k_itemsets}-itemsets candidates\", colour=\"yellow\"):\n",
        "            for j in range(i + 1, len(current_itemsets)):\n",
        "                # Try to join two itemsets\n",
        "                candidate = current_itemsets[i] | current_itemsets[j]\n",
        "                if len(candidate) == k_itemsets:  # Only consider itemsets of size k_itemsets\n",
        "                    # Candidate itemset must not have any infrequent subsets\n",
        "                    if all(frozenset(comb) in itemsets for comb in combinations(candidate, k_itemsets - 1)):\n",
        "                        for doc in dataset:\n",
        "                            if candidate.issubset(doc):\n",
        "                                candidate_itemsets[candidate] += 1\n",
        "\n",
        "        # Prune infrequent itemsets\n",
        "        candidate_itemsets = {itemset: count for itemset, count in candidate_itemsets.items() if count >= min_support}\n",
        "\n",
        "        if not candidate_itemsets:\n",
        "            break  # No more frequent itemsets can be generated\n",
        "\n",
        "        # Update the frequent itemsets dictionary with only size k itemsets\n",
        "        if k_itemsets == k:\n",
        "            frequent_itemsets.update(candidate_itemsets)\n",
        "\n",
        "        # Update current itemsets for the next iteration\n",
        "        current_itemsets = list(candidate_itemsets.keys())\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "\n",
        "def main(path, name, k, min_support):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize File class to load data\n",
        "    file_handler = File(path, name)\n",
        "\n",
        "    # Load data\n",
        "    file_handler.load_data()\n",
        "\n",
        "    # Get the list of documents as transactions\n",
        "    documents = [file_handler.get_words_by_docID(doc_id) for doc_id in file_handler.get_unique_docs()]\n",
        "\n",
        "    # Apply Apriori algorithm to find frequent itemsets of size k\n",
        "    print(\"Running Apriori algorithm...\")\n",
        "    frequent_itemsets = apriori(documents, min_support, k)\n",
        "\n",
        "    # Print the results for frequent itemsets of size k\n",
        "    print(\"\\nFrequent Itemsets of size\", k, \":\")\n",
        "    if frequent_itemsets:\n",
        "        for itemset, count in tqdm(frequent_itemsets.items(), desc=\"Printing itemsets\", colour=\"blue\"):\n",
        "            print(f\"Itemset: {set(itemset)}, Support: {count}\")\n",
        "    else:\n",
        "        print(f\"No frequent itemsets of size {k} found.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    dataset_name = input(\"Enter dataset name (e.g., enron): \")\n",
        "    k = int(input(\"Enter k (size of itemsets): \"))\n",
        "    min_support = int(input(\"Enter min_support (minimum frequency threshold): \"))\n",
        "    path = r\"/content/drive/MyDrive/bag+of+words\"  # Adjust path if needed\n",
        "    main(path, dataset_name, k, min_support)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNwrImaULEea",
        "outputId": "9ac80e59-04fc-4568-cbb2-6c4c0de11d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter dataset name (e.g., enron): nips\n",
            "Enter k (size of itemsets): 3\n",
            "Enter min_support (minimum frequency threshold): 900\n",
            "Reading vocabulary file...\n",
            "Vocabulary file loaded. Time taken: 0.01 seconds.\n",
            "Reading docword file (metadata)...\n",
            "Docword file metadata loaded. Time taken: 0.01 seconds.\n",
            "Reading docword file (data)...\n",
            "\n",
            "\n",
            "Docword file data loaded. Time taken: 0.48 seconds.\n",
            "Running Apriori algorithm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 1-itemsets: 100%|\u001b[32m██████████\u001b[0m| 1500/1500 [00:00<00:00, 3368.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating 2-itemsets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 2-itemsets candidates: 100%|\u001b[33m██████████\u001b[0m| 50/50 [00:51<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating 3-itemsets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 3-itemsets candidates: 100%|\u001b[33m██████████\u001b[0m| 365/365 [00:00<00:00, 13984.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Frequent Itemsets of size 3 :\n",
            "No frequent itemsets of size 3 found.\n",
            "Total Time taken: 53.79 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code run for k = 3, f = 500 with output on nips dataset"
      ],
      "metadata": {
        "id": "sq4vXSQjdQnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "import time\n",
        "from pathlib import Path\n",
        "import os\n",
        "import errno\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "\n",
        "class File:\n",
        "    def __init__(self, path, name):\n",
        "        self.name = str(name)\n",
        "        self.vocabFile = \"vocab.\" + self.name + \".txt\"\n",
        "        self.vocabPath = Path(path) / self.vocabFile\n",
        "        self.docWordFile = \"docword.\" + self.name + \".txt.gz\"\n",
        "        self.docWordPath = Path(path) / self.docWordFile\n",
        "\n",
        "        if not self.vocabPath.exists() or self.vocabPath.is_dir():\n",
        "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.vocabPath)\n",
        "        if not self.docWordPath.exists() or self.docWordPath.is_dir():\n",
        "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.docWordPath)\n",
        "\n",
        "    def __read_csv_chunks(self, lines, path, **params):\n",
        "        chunks = []\n",
        "        for i, chunk in enumerate(pd.read_csv(path, **params)):\n",
        "            prog = min((i + 1) / lines * 100.0 * params[\"chunksize\"], 100)\n",
        "            print(f\"Data Load Progress : {prog:.2f} %.\", end=\"\\r\", flush=True)\n",
        "            chunks.append(chunk)\n",
        "        print(\"\\n\")\n",
        "        df = pd.concat(chunks, axis=0)\n",
        "        del chunks\n",
        "        return df\n",
        "\n",
        "    def load_data(self):\n",
        "        start_time = time.time()\n",
        "        print(\"Reading vocabulary file...\")\n",
        "        self.vocab = pd.read_csv(self.vocabPath, header=None, names=[\"word\"])\n",
        "        print(f\"Vocabulary file loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        self.vocab.index += 1\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"Reading docword file (metadata)...\")\n",
        "        tmp = pd.read_csv(self.docWordPath, compression='gzip', header=None, nrows=3)\n",
        "        self.docCount = tmp[0].values[0]\n",
        "        self.wordCount = tmp[0].values[1]\n",
        "        self.NNZ = tmp[0].values[2]\n",
        "        print(f\"Docword file metadata loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"Reading docword file (data)...\")\n",
        "        self.docWord = self.__read_csv_chunks(self.NNZ, self.docWordPath, compression='gzip', header=None, sep=' ',\n",
        "                                              quotechar='\"', on_bad_lines=\"skip\", skiprows=3, chunksize=10000,\n",
        "                                              names=[\"docID\", \"wordID\", \"count\"])\n",
        "        print(f\"Docword file data loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    def get_unique_docs(self):\n",
        "        try:\n",
        "            return self.docIDS\n",
        "        except AttributeError:\n",
        "            docIDS = self.docWord[\"docID\"].unique()\n",
        "            docIDS.sort()\n",
        "            self.docIDS = docIDS\n",
        "            return self.docIDS\n",
        "\n",
        "    def get_words_by_docID(self, _id):\n",
        "        try:\n",
        "            return self.docWordList[_id]\n",
        "        except AttributeError:\n",
        "            return self.docWord[self.docWord['docID'] == _id][\"wordID\"].tolist()\n",
        "\n",
        "\n",
        "# Optimized Apriori algorithm to find frequent itemsets of size k\n",
        "def apriori(dataset, min_support, k):\n",
        "    itemsets = defaultdict(int)\n",
        "\n",
        "    # Generate all itemsets of size 1 (frequent 1-itemsets)\n",
        "    for doc in tqdm(dataset, desc=\"Generating 1-itemsets\", colour=\"green\"):\n",
        "        for word in doc:\n",
        "            itemsets[frozenset([word])] += 1\n",
        "\n",
        "    # Prune infrequent itemsets\n",
        "    itemsets = {itemset: count for itemset, count in itemsets.items() if count >= min_support}\n",
        "\n",
        "    # Start generating k-itemsets\n",
        "    frequent_itemsets = defaultdict(int)\n",
        "    current_itemsets = list(itemsets.keys())\n",
        "\n",
        "    # We only need to keep itemsets of size k, so we stop the generation process once we reach k-itemsets.\n",
        "    for k_itemsets in range(2, k + 1):\n",
        "        print(f\"\\nGenerating {k_itemsets}-itemsets...\")\n",
        "\n",
        "        # Generate candidate itemsets from the previous frequent itemsets\n",
        "        candidate_itemsets = defaultdict(int)\n",
        "\n",
        "        # Join itemsets of size (k_itemsets - 1) to generate k_itemsets\n",
        "        for i in tqdm(range(len(current_itemsets)), desc=f\"Generating {k_itemsets}-itemsets candidates\", colour=\"yellow\"):\n",
        "            for j in range(i + 1, len(current_itemsets)):\n",
        "                # Try to join two itemsets\n",
        "                candidate = current_itemsets[i] | current_itemsets[j]\n",
        "                if len(candidate) == k_itemsets:  # Only consider itemsets of size k_itemsets\n",
        "                    # Candidate itemset must not have any infrequent subsets\n",
        "                    if all(frozenset(comb) in itemsets for comb in combinations(candidate, k_itemsets - 1)):\n",
        "                        for doc in dataset:\n",
        "                            if candidate.issubset(doc):\n",
        "                                candidate_itemsets[candidate] += 1\n",
        "\n",
        "        # Prune infrequent itemsets\n",
        "        candidate_itemsets = {itemset: count for itemset, count in candidate_itemsets.items() if count >= min_support}\n",
        "\n",
        "        if not candidate_itemsets:\n",
        "            break  # No more frequent itemsets can be generated\n",
        "\n",
        "        # Update the frequent itemsets dictionary with only size k itemsets\n",
        "        if k_itemsets == k:\n",
        "            frequent_itemsets.update(candidate_itemsets)\n",
        "\n",
        "        # Update current itemsets for the next iteration\n",
        "        current_itemsets = list(candidate_itemsets.keys())\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "\n",
        "def main(path, name, k, min_support):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize File class to load data\n",
        "    file_handler = File(path, name)\n",
        "\n",
        "    # Load data\n",
        "    file_handler.load_data()\n",
        "\n",
        "    # Get the list of documents as transactions\n",
        "    documents = [file_handler.get_words_by_docID(doc_id) for doc_id in file_handler.get_unique_docs()]\n",
        "\n",
        "    # Apply Apriori algorithm to find frequent itemsets of size k\n",
        "    print(\"Running Apriori algorithm...\")\n",
        "    frequent_itemsets = apriori(documents, min_support, k)\n",
        "\n",
        "    # Print the results for frequent itemsets of size k\n",
        "    print(\"\\nFrequent Itemsets of size\", k, \":\")\n",
        "    if frequent_itemsets:\n",
        "        for itemset, count in tqdm(frequent_itemsets.items(), desc=\"Printing itemsets\", colour=\"blue\"):\n",
        "            print(f\"Itemset: {set(itemset)}, Support: {count}\")\n",
        "    else:\n",
        "        print(f\"No frequent itemsets of size {k} found.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    dataset_name = input(\"Enter dataset name (e.g., enron): \")\n",
        "    k = int(input(\"Enter k (size of itemsets): \"))\n",
        "    min_support = int(input(\"Enter min_support (minimum frequency threshold): \"))\n",
        "    path = r\"/content/drive/MyDrive/bag+of+words\"  # Adjust path if needed\n",
        "    main(path, dataset_name, k, min_support)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAaEQNwSM-tO",
        "outputId": "0aa59019-0dba-4f0e-9619-5c95a8c94430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter dataset name (e.g., enron): nips\n",
            "Enter k (size of itemsets): 3\n",
            "Enter min_support (minimum frequency threshold): 500\n",
            "Reading vocabulary file...\n",
            "Vocabulary file loaded. Time taken: 0.01 seconds.\n",
            "Reading docword file (metadata)...\n",
            "Docword file metadata loaded. Time taken: 0.01 seconds.\n",
            "Reading docword file (data)...\n",
            "\n",
            "\n",
            "Docword file data loaded. Time taken: 0.44 seconds.\n",
            "Running Apriori algorithm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 1-itemsets: 100%|\u001b[32m██████████\u001b[0m| 1500/1500 [00:00<00:00, 3278.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating 2-itemsets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 2-itemsets candidates: 100%|\u001b[33m██████████\u001b[0m| 252/252 [21:34<00:00,  5.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating 3-itemsets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 3-itemsets candidates: 100%|\u001b[33m██████████\u001b[0m| 5521/5521 [00:04<00:00, 1330.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Frequent Itemsets of size 3 :\n",
            "No frequent itemsets of size 3 found.\n",
            "Total Time taken: 1301.73 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code run for k = 2, f = 1000 with output on enron dataset\n",
        "\n",
        "(not completed as it took too much time)"
      ],
      "metadata": {
        "id": "dVodC6JWdWx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "import time\n",
        "from pathlib import Path\n",
        "import os\n",
        "import errno\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "\n",
        "class File:\n",
        "    def __init__(self, path, name):\n",
        "        self.name = str(name)\n",
        "        self.vocabFile = \"vocab.\" + self.name + \".txt\"\n",
        "        self.vocabPath = Path(path) / self.vocabFile\n",
        "        self.docWordFile = \"docword.\" + self.name + \".txt.gz\"\n",
        "        self.docWordPath = Path(path) / self.docWordFile\n",
        "\n",
        "        if not self.vocabPath.exists() or self.vocabPath.is_dir():\n",
        "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.vocabPath)\n",
        "        if not self.docWordPath.exists() or self.docWordPath.is_dir():\n",
        "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.docWordPath)\n",
        "\n",
        "    def __read_csv_chunks(self, lines, path, **params):\n",
        "        chunks = []\n",
        "        for i, chunk in enumerate(pd.read_csv(path, **params)):\n",
        "            prog = min((i + 1) / lines * 100.0 * params[\"chunksize\"], 100)\n",
        "            print(f\"Data Load Progress : {prog:.2f} %.\", end=\"\\r\", flush=True)\n",
        "            chunks.append(chunk)\n",
        "        print(\"\\n\")\n",
        "        df = pd.concat(chunks, axis=0)\n",
        "        del chunks\n",
        "        return df\n",
        "\n",
        "    def load_data(self):\n",
        "        start_time = time.time()\n",
        "        print(\"Reading vocabulary file...\")\n",
        "        self.vocab = pd.read_csv(self.vocabPath, header=None, names=[\"word\"])\n",
        "        print(f\"Vocabulary file loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        self.vocab.index += 1\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"Reading docword file (metadata)...\")\n",
        "        tmp = pd.read_csv(self.docWordPath, compression='gzip', header=None, nrows=3)\n",
        "        self.docCount = tmp[0].values[0]\n",
        "        self.wordCount = tmp[0].values[1]\n",
        "        self.NNZ = tmp[0].values[2]\n",
        "        print(f\"Docword file metadata loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        print(\"Reading docword file (data)...\")\n",
        "        self.docWord = self.__read_csv_chunks(self.NNZ, self.docWordPath, compression='gzip', header=None, sep=' ',\n",
        "                                              quotechar='\"', on_bad_lines=\"skip\", skiprows=3, chunksize=10000,\n",
        "                                              names=[\"docID\", \"wordID\", \"count\"])\n",
        "        print(f\"Docword file data loaded. Time taken: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    def get_unique_docs(self):\n",
        "        try:\n",
        "            return self.docIDS\n",
        "        except AttributeError:\n",
        "            docIDS = self.docWord[\"docID\"].unique()\n",
        "            docIDS.sort()\n",
        "            self.docIDS = docIDS\n",
        "            return self.docIDS\n",
        "\n",
        "    def get_words_by_docID(self, _id):\n",
        "        try:\n",
        "            return self.docWordList[_id]\n",
        "        except AttributeError:\n",
        "            return self.docWord[self.docWord['docID'] == _id][\"wordID\"].tolist()\n",
        "\n",
        "\n",
        "# Optimized Apriori algorithm to find frequent itemsets of size k\n",
        "def apriori(dataset, min_support, k):\n",
        "    itemsets = defaultdict(int)\n",
        "\n",
        "    # Generate all itemsets of size 1 (frequent 1-itemsets)\n",
        "    for doc in tqdm(dataset, desc=\"Generating 1-itemsets\", colour=\"green\"):\n",
        "        for word in doc:\n",
        "            itemsets[frozenset([word])] += 1\n",
        "\n",
        "    # Prune infrequent itemsets\n",
        "    itemsets = {itemset: count for itemset, count in itemsets.items() if count >= min_support}\n",
        "\n",
        "    # Start generating k-itemsets\n",
        "    frequent_itemsets = defaultdict(int)\n",
        "    current_itemsets = list(itemsets.keys())\n",
        "\n",
        "    # We only need to keep itemsets of size k, so we stop the generation process once we reach k-itemsets.\n",
        "    for k_itemsets in range(2, k + 1):\n",
        "        print(f\"\\nGenerating {k_itemsets}-itemsets...\")\n",
        "\n",
        "        # Generate candidate itemsets from the previous frequent itemsets\n",
        "        candidate_itemsets = defaultdict(int)\n",
        "\n",
        "        # Join itemsets of size (k_itemsets - 1) to generate k_itemsets\n",
        "        for i in tqdm(range(len(current_itemsets)), desc=f\"Generating {k_itemsets}-itemsets candidates\", colour=\"yellow\"):\n",
        "            for j in range(i + 1, len(current_itemsets)):\n",
        "                # Try to join two itemsets\n",
        "                candidate = current_itemsets[i] | current_itemsets[j]\n",
        "                if len(candidate) == k_itemsets:  # Only consider itemsets of size k_itemsets\n",
        "                    # Candidate itemset must not have any infrequent subsets\n",
        "                    if all(frozenset(comb) in itemsets for comb in combinations(candidate, k_itemsets - 1)):\n",
        "                        for doc in dataset:\n",
        "                            if candidate.issubset(doc):\n",
        "                                candidate_itemsets[candidate] += 1\n",
        "\n",
        "        # Prune infrequent itemsets\n",
        "        candidate_itemsets = {itemset: count for itemset, count in candidate_itemsets.items() if count >= min_support}\n",
        "\n",
        "        if not candidate_itemsets:\n",
        "            break  # No more frequent itemsets can be generated\n",
        "\n",
        "        # Update the frequent itemsets dictionary with only size k itemsets\n",
        "        if k_itemsets == k:\n",
        "            frequent_itemsets.update(candidate_itemsets)\n",
        "\n",
        "        # Update current itemsets for the next iteration\n",
        "        current_itemsets = list(candidate_itemsets.keys())\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "\n",
        "def main(path, name, k, min_support):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize File class to load data\n",
        "    file_handler = File(path, name)\n",
        "\n",
        "    # Load data\n",
        "    file_handler.load_data()\n",
        "\n",
        "    # Get the list of documents as transactions\n",
        "    documents = [file_handler.get_words_by_docID(doc_id) for doc_id in file_handler.get_unique_docs()]\n",
        "\n",
        "    # Apply Apriori algorithm to find frequent itemsets of size k\n",
        "    print(\"Running Apriori algorithm...\")\n",
        "    frequent_itemsets = apriori(documents, min_support, k)\n",
        "\n",
        "    # Print the results for frequent itemsets of size k\n",
        "    print(\"\\nFrequent Itemsets of size\", k, \":\")\n",
        "    if frequent_itemsets:\n",
        "        for itemset, count in tqdm(frequent_itemsets.items(), desc=\"Printing itemsets\", colour=\"blue\"):\n",
        "            print(f\"Itemset: {set(itemset)}, Support: {count}\")\n",
        "    else:\n",
        "        print(f\"No frequent itemsets of size {k} found.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Total Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    dataset_name = input(\"Enter dataset name (e.g., enron): \")\n",
        "    k = int(input(\"Enter k (size of itemsets): \"))\n",
        "    min_support = int(input(\"Enter min_support (minimum frequency threshold): \"))\n",
        "    path = r\"/content/drive/MyDrive/bag+of+words\"  # Adjust path if needed\n",
        "    main(path, dataset_name, k, min_support)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "sv76sIaYOzCL",
        "outputId": "be663463-8047-4435-d115-7113f77ddd8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter dataset name (e.g., enron): enron\n",
            "Enter k (size of itemsets): 2\n",
            "Enter min_support (minimum frequency threshold): 1000\n",
            "Reading vocabulary file...\n",
            "Vocabulary file loaded. Time taken: 0.69 seconds.\n",
            "Reading docword file (metadata)...\n",
            "Docword file metadata loaded. Time taken: 0.67 seconds.\n",
            "Reading docword file (data)...\n",
            "\n",
            "\n",
            "Docword file data loaded. Time taken: 3.51 seconds.\n",
            "Running Apriori algorithm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 1-itemsets: 100%|\u001b[32m██████████\u001b[0m| 39861/39861 [00:02<00:00, 15316.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating 2-itemsets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating 2-itemsets candidates:   1%|\u001b[33m▏         \u001b[0m| 10/789 [28:46<37:21:17, 172.63s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-0fac7d94ede5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mmin_support\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter min_support (minimum frequency threshold): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"/content/drive/MyDrive/bag+of+words\"\u001b[0m  \u001b[0;31m# Adjust path if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_support\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-0fac7d94ede5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(path, name, k, min_support)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;31m# Apply Apriori algorithm to find frequent itemsets of size k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running Apriori algorithm...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mfrequent_itemsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapriori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_support\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m# Print the results for frequent itemsets of size k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-0fac7d94ede5>\u001b[0m in \u001b[0;36mapriori\u001b[0;34m(dataset, min_support, k)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitemsets\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcomb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_itemsets\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                             \u001b[0;32mif\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                                 \u001b[0mcandidate_itemsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}